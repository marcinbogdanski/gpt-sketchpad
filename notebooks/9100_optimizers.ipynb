{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf24e9aa",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Implementations for SDG, Momentum, etc. up to AdamW and Muon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c56216",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n",
    "\n",
    "**Idea:** Take a step in the negative gradient direction.\n",
    "\n",
    "```\n",
    "p = p - lr * g\n",
    "```\n",
    "\n",
    "That's it. Simple, but struggles with noisy gradients and ill-conditioned landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f553eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "                # Update\n",
    "                p.data.add_(grad, alpha=-group['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ab2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr, weight_decay=0.01)\n",
    "opt_custom = SDG([W2], lr=lr, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a765b",
   "metadata": {},
   "source": [
    "## SGD with Momentum\n",
    "\n",
    "**Idea:** Accumulate gradients over time into a \"velocity.\" Smooths out noise, builds up speed in consistent directions.\n",
    "\n",
    "```\n",
    "v = B * v + g\n",
    "p = p - lr * v\n",
    "```\n",
    "\n",
    "Typical B = 0.9 (averages ~10 steps). Steady-state velocity is g/(1-B), so effective step is larger than vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDGMomentum(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'momentum_buffer': torch.zeros_like(p),\n",
    "                    }\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "                # Update Step\n",
    "                v = self.state[p]['momentum_buffer']\n",
    "                v.mul_(group['momentum']).add_(grad)\n",
    "                p.data.add_(v, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr, momentum=0.9, weight_decay=0.01)\n",
    "opt_custom = SDGMomentum([W2], lr=lr, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['momentum_buffer']\n",
    "    assert torch.equal(state1['momentum_buffer'], state2['momentum_buffer'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15721b0b",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "**Idea:** Fix AdaGrad by using EMA instead of sum. Old gradients decay away.\n",
    "\n",
    "```\n",
    "s = B * s + (1 - B) * g^2\n",
    "p = p - lr * g / (sqrt(s) + eps)\n",
    "```\n",
    "\n",
    "Typical B = 0.99. Learning rate stabilizes instead of decaying to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'square_avg': torch.zeros_like(p),\n",
    "                    }\n",
    "\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "\n",
    "                # Update Step\n",
    "                s = self.state[p]['square_avg']\n",
    "                s.mul_(group['alpha'])\n",
    "\n",
    "                # s = s + (1-group['alpha']) * grad * grad\n",
    "                s.addcmul_(grad, grad, value=1-group['alpha'])\n",
    "\n",
    "                avg = s.sqrt().add_(group['eps'])\n",
    "                \n",
    "                # p.add_(grad / avg, alpha=-group['lr'])\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.RMSprop([W1], lr=lr, alpha=0.99, weight_decay=0.01)\n",
    "opt_custom = RMSProp([W2], lr=lr, alpha=0.99, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['step', 'square_avg']\n",
    "    assert torch.equal(state1['square_avg'], state2['square_avg'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e4f31",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Idea:** Combine momentum (first moment) with RMSprop (second moment). Add bias correction for early steps.\n",
    "\n",
    "```\n",
    "v = B1 * v + (1 - B1) * g          # first moment (direction)\n",
    "s = B2 * s + (1 - B2) * g^2        # second moment (scaling)\n",
    "\n",
    "v_corrected = v / (1 - B1^t)       # bias correction\n",
    "s_corrected = s / (1 - B2^t)\n",
    "\n",
    "p = p - lr * v_corrected / (sqrt(s_corrected) + eps)\n",
    "p = p - lr * wd * p                # AdamW: decoupled weight decay\n",
    "```\n",
    "\n",
    "Typical: B1 = 0.9, B2 = 0.999, eps = 1e-8.\n",
    "\n",
    "Bias correction compensates for zero initialization. After ~1000 steps, correction ≈ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7111ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, decoupled_weight_decay=False):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, decoupled_weight_decay=decoupled_weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'step': 0,\n",
    "                        'exp_avg': torch.zeros_like(p),\n",
    "                        'exp_avg_sq': torch.zeros_like(p),\n",
    "                    }\n",
    "                self.state[p]['step'] += 1\n",
    "\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    if not group['decoupled_weight_decay']:\n",
    "                        # Adam\n",
    "                        # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                        grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "                    else:\n",
    "                        # AdamW\n",
    "                        # p = p - lr * weight_decay * p\n",
    "                        p.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "\n",
    "                # Update v\n",
    "                # v = B1 * v + (1-B1) * g\n",
    "                v = self.state[p]['exp_avg']\n",
    "                v.lerp_(grad, 1 - group['betas'][0])\n",
    "\n",
    "                # Update s\n",
    "                # s = B2 * s + (1-B2) * g**2\n",
    "                s = self.state[p]['exp_avg_sq']\n",
    "                s.mul_(group['betas'][1])\n",
    "                s.addcmul_(grad, grad, value=1-group['betas'][1])\n",
    "\n",
    "                # Correction\n",
    "                # Somewhat convoluted way to do:\n",
    "                # v_corrected = v / (1-B1**t)\n",
    "                # s_corrected = s / (1-B2**t)\n",
    "                # p = p - lr * v_corrected / (sqrt(s_corrected)+eps)\n",
    "                t = self.state[p]['step']\n",
    "                bias1 = 1-group['betas'][0]**t\n",
    "                bias2 = 1-group['betas'][1]**t\n",
    "                bias2_sqrt = bias2**0.5\n",
    "                denom = (s.sqrt() / bias2_sqrt).add_(group['eps'])\n",
    "                p.data.addcdiv_(v, denom, value=-group['lr'] / bias1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d196c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.Adam([W1], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "opt_custom = Adam([W2], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['step', 'exp_avg', 'exp_avg_sq']\n",
    "    assert state1['step'] == state2['step']\n",
    "    assert torch.equal(state1['exp_avg'], state2['exp_avg'])\n",
    "    assert torch.equal(state1['exp_avg_sq'], state2['exp_avg_sq'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2060d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.AdamW([W1], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "opt_custom = Adam([W2], lr=lr, betas=(0.9, 0.999), weight_decay=0.01, decoupled_weight_decay=True)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['step', 'exp_avg', 'exp_avg_sq']\n",
    "    assert state1['step'] == state2['step']\n",
    "    assert torch.equal(state1['exp_avg'], state2['exp_avg'])\n",
    "    assert torch.equal(state1['exp_avg_sq'], state2['exp_avg_sq'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35898298",
   "metadata": {},
   "source": [
    "## Newton-Schulz Iteration\n",
    "\n",
    "**Not an optimizer**—a subroutine to orthogonalize a matrix cheaply.\n",
    "\n",
    "**Problem:** Given matrix G, find closest orthogonal matrix U. SVD works but is O(n³) and slow on GPU.\n",
    "\n",
    "**Solution:** Iterative approximation.\n",
    "\n",
    "```\n",
    "X = G / ||G||                        # scale so singular values < 1\n",
    "\n",
    "repeat 5 times:\n",
    "    X = 1.5 * X - 0.5 * X @ X.T @ X\n",
    "\n",
    "return X\n",
    "```\n",
    "\n",
    "Converges to orthogonal matrix. Each iteration is just matrix multiplies-GPU friendly.\n",
    "\n",
    "For non-square matrices, adjust multiplication order based on shape (orthonormalize the smaller dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropower_via_newtonschulz(grad, steps=5):\n",
    "    assert grad.ndim == 2\n",
    "    a, b, c = 3.4445, -4.7750, 2.0315\n",
    "    eps=1e-7\n",
    "    X = grad.bfloat16()\n",
    "    if grad.size(0) > grad.size(1):\n",
    "        X = X.T\n",
    "    # Scale down to norm at most 1\n",
    "    X.div_(X.norm().clamp(min=eps))\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = torch.addmm(A, A, A, beta=b, alpha=c)\n",
    "        X = torch.addmm(X, B, X, beta=a)\n",
    "    if grad.size(0) > grad.size(1):\n",
    "        X = X.T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = torch.randn(128, 64)\n",
    "X = zeropower_via_newtonschulz(grad, steps=5)\n",
    "assert X.shape == grad.shape\n",
    "# Short side orthogonality: X.T @ X should be ~I since n < m\n",
    "eye = X.float().T @ X.float()\n",
    "# off-diagonals are small\n",
    "assert (eye - torch.diag(eye.diag())).abs().max() < 0.2\n",
    "# diagonal is in reasonable range\n",
    "assert eye.diag().min() > 0.5 and eye.diag().max() < 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e399a9",
   "metadata": {},
   "source": [
    "## Muon\n",
    "\n",
    "**Idea:** Replace Adam's element-wise scaling with matrix-level orthogonalization. Each update is \"balanced\" - no direction gets special treatment.\n",
    "\n",
    "```\n",
    "p = p - lr * wd * p                # decoupled weight decay\n",
    "\n",
    "v = B * v + (1-B) * g              # momentum or EMA equivalent, because orthogonalization wipes scale\n",
    "                                   # EMA possibly more numerical stable\n",
    "\n",
    "vv = B * v + (1-B) * g             # optional, Nesterov look-ahead (note it's just lerp again)\n",
    "\n",
    "U = newton_schulz(vv)              # orthogonalize\n",
    "\n",
    "lr_adj = lr * sqrt(max(1, m/n))    # adjust for aspect ratio\n",
    "p = p - lr * U\n",
    "```\n",
    "\n",
    "Typical: B = 0.95, lr = 0.02, 5 Newton-Schulz iterations.\n",
    "\n",
    "**Key insight:** Adam treats weight matrices as bags of independent numbers. Muon treats them as transformations with structure. Orthogonalization preserves the \"direction\" of the update while balancing magnitudes across the matrix.\n",
    "\n",
    "**Scope:** Only applies to 2D weight matrices. Use Adam for embeddings, biases, LayerNorm params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f867a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.95, nesterov=True, ns_steps=5, weight_decay=0.1):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'momentum_buffer': torch.zeros_like(p),\n",
    "                    }\n",
    "\n",
    "                # Decoupled Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "\n",
    "                # Update v\n",
    "                # v = B1 * v + (1-B) * g\n",
    "                v = self.state[p]['momentum_buffer']\n",
    "                v.lerp_(grad, 1 - group['momentum'])\n",
    "\n",
    "                # Optional Nesterov look-ahead\n",
    "                # vv = B*v + (1-B)*g\n",
    "                vv = grad.lerp(v, group['momentum']) if group['nesterov'] else v\n",
    "\n",
    "                # Update\n",
    "                update = zeropower_via_newtonschulz(vv, group['ns_steps'])\n",
    "                lr = group['lr'] * (max(1, p.size(0) / p.size(1)))**0.5\n",
    "                p.add_(update, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "momentum = 0.95\n",
    "nesterov = True\n",
    "ns_steps = 5\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.Muon([W1], lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps,\n",
    "                             weight_decay=0.01, adjust_lr_fn=\"original\")\n",
    "opt_custom = Muon([W2], lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['momentum_buffer']\n",
    "    assert torch.equal(state1['momentum_buffer'], state2['momentum_buffer'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc9ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
