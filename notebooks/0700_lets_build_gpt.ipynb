{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "460bfb73",
   "metadata": {},
   "source": [
    "# Let's Build GPT\n",
    "\n",
    "Data: https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc534331",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da4e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba96ba98",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.n_vocab = len(vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d317bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chars: 1115394\n",
      "Dataset Start:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/tinyshakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(\"Num chars:\", len(text))\n",
    "print(\"Dataset Start:\")\n",
    "print(text[:462])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db228748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Num: 65\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(text))))\n",
    "print(''.join(letters))\n",
    "print('Num:', len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ae424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n",
      "Newline is: 0\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer(letters)\n",
    "print(tok.encode(\"hii there\"))\n",
    "print(tok.decode(tok.encode(\"hii there\")))\n",
    "print(f\"Newline is: {tok.encode('\\n')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len: 1003854\n",
      "Valid data len: 111540\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tok.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(text))\n",
    "train_data, eval_data = data[:n], data[n:]  # 90%/10% split \n",
    "print(f\"Train data len: {len(train_data)}\")\n",
    "print(f\"Valid data len: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data, batch_size, sequence_length, device):\n",
    "        self.data = data\n",
    "        self.n_batch = batch_size\n",
    "        self.n_seq = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "    def get_batch(self):\n",
    "        bi = torch.randint(len(self.data)-self.n_seq, (self.n_batch,))\n",
    "        x = torch.stack([self.data[i:i+self.n_seq] for i in bi])\n",
    "        y = torch.stack([self.data[i+1:i+1+self.n_seq] for i in bi])\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfcbe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "tr_data_loader = DataLoader(train_data, batch_size=4, sequence_length=8, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b85b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = tr_data_loader.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb65ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x_batch.shape)\n",
    "print(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47abe159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y_batch.shape)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef86d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One self-attention head\"\"\"\n",
    "    def __init__(self, n_seq, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.key = nn.Linear(n_embd, n_head, bias=False)\n",
    "        self.query = nn.Linear(n_embd, n_head, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_head, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((n_seq, n_seq))))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        x_key = self.key(x)    # B,T,H\n",
    "        x_query = self.query(x)  # B,T,H\n",
    "        x_value = self.value(x)  # B,T,H\n",
    "\n",
    "        H = x_key.shape[-1]\n",
    "        W_affin = x_query @ x_key.mT / H**0.5 # / H**0.5  # B,T,T <- B,T,C @ B,C,T\n",
    "        W_affin = W_affin.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        W_affin = torch.softmax(W_affin, dim=-1)\n",
    "\n",
    "        W_affin = self.drop(W_affin)\n",
    "\n",
    "        out = W_affin @ x_value\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a536ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    \"\"\"Multiple self-attention heads\"\"\"\n",
    "    def __init__(self, n_seq, n_embd, head_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(n_seq, n_embd, head_size, dropout) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "75430384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Linear transform and activation\"\"\"\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_embd, 4*n_embd)\n",
    "        self.act = nn.ReLU()\n",
    "        self.proj = nn.Linear(4*n_embd, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = self.act(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4c75ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_seq, n_embd, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa_heads = MultiHead(\n",
    "            n_seq, n_embd=n_embd, head_size=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))        # B,T,E pre-norm\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "24c7c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_seq, n_vocab, n_embd, num_heads, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.tok_emb_table = nn.Embedding(n_vocab, n_embd)\n",
    "        self.pos_emb_table = nn.Embedding(n_seq, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_seq=n_seq, n_embd=n_embd, num_heads=num_heads, dropout=dropout) for _ in range(n_layer)]            \n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, n_vocab)\n",
    "\n",
    "        # better init - sneaky addition in the Karpathy repo, not in the video\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    # def _init_weights(self, module):\n",
    "    #     if isinstance(module, nn.Linear):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #         if module.bias is not None:\n",
    "    #             torch.nn.init.zeros_(module.bias)\n",
    "    #     elif isinstance(module, nn.Embedding):\n",
    "    #         torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        assert idx.dtype == torch.long\n",
    "        assert targets is None or targets.dtype == torch.long\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.tok_emb_table(idx)    #  B,T,E <- B,T\n",
    "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))    #  B,T,E <- B,T\n",
    "        x = tok_emb + pos_emb  #  B,T,E\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)   # B,T,V <- B,T,E\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_ = logits.view(B*T, C)  # B*T, C\n",
    "            targets_ = targets.view(B*T)   # B*T\n",
    "            loss = F.cross_entropy(logits_, targets_)\n",
    "            return logits, loss\n",
    "    \n",
    "    def generate(self, idx, n_seq, max_tokens):\n",
    "        \"\"\"Generate max_tokens starting from idx[B,T]\"\"\"\n",
    "        # assert idx.shape == (n_batch, n_seq)\n",
    "        assert idx.dtype == torch.long\n",
    "        assert isinstance(max_tokens, int)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "\n",
    "            # Sliding window over idx\n",
    "            idx_tail = idx[:, -n_seq:]\n",
    "\n",
    "            # Model Output\n",
    "            logits, _ = self(idx_tail)      # B,T,C <- B,T\n",
    "\n",
    "            # Discard all but last step\n",
    "            logits = logits[:, -1, :]  # B,C <- B,T,C\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # B, T+1\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "51323529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected initial loss: 4.174387454986572\n",
      "Initial model loss: 4.351653099060059\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Hyperparameters\n",
    "n_vocab = tok.n_vocab   # num letters, token dictionary size\n",
    "n_batch = 64            # mini-bach, how many in parallel\n",
    "n_seq = 256             # max context length, max len feed into the model\n",
    "n_embd = 384             # size of embeddings, i.e. 'first layer'\n",
    "num_heads = 6     # head size 384/6=64\n",
    "num_layer = 6\n",
    "dropout = 0.2\n",
    "lr = 3e-4\n",
    "\n",
    "\n",
    "# Data Loaders\n",
    "tr_data_loader = DataLoader(train_data, n_batch, n_seq, device)\n",
    "ev_data_loader = DataLoader(eval_data, n_batch, n_seq, device)\n",
    "\n",
    "# Model\n",
    "model = TransformerModel(n_seq, n_vocab, n_embd, num_heads, num_layer, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Initial Loss\n",
    "print(f\"Expected initial loss: {-torch.tensor(1/n_vocab).log()}\")\n",
    "logits, loss = model(x_batch, y_batch)\n",
    "print(f\"Initial model loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "28f09713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SvgO-3IjM:d?gLTa\n",
      "hX:YVXJFpXMNuwqcBMIG.tbfr dXlaDZaLeHfwHcHwwRWQ,fDEZaYuxznIoQX\n",
      "Yo3&$-MtofCizIIBb!&V!\n"
     ]
    }
   ],
   "source": [
    "# Example Generation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    idx = torch.tensor([[0]], device=device)  # B=1, T=1, '\\n'\n",
    "    res = model.generate(idx, n_seq, max_tokens=100)\n",
    "    print(tok.decode(res[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d1103751",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(data_loader, num_evals, device=device):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(num_evals, device=device)\n",
    "    for i in range(num_evals):\n",
    "        x_batch, y_batch = data_loader.get_batch()\n",
    "        _, loss = model(x_batch, y_batch)\n",
    "        losses[i] = loss\n",
    "    return losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ad4426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "eval_every = 1000\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1796e9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=15.12s i=0, l_running=4.2892, tr=3.5284 ev=3.5495\n",
      "t=132.02s i=1000, l_running=1.5871, tr=1.5329 ev=1.7182\n",
      "t=132.43s i=2000, l_running=1.3890, tr=1.3042 ev=1.5461\n",
      "t=131.78s i=3000, l_running=1.2771, tr=1.2008 ev=1.4925\n",
      "t=131.85s i=4000, l_running=1.2001, tr=1.1201 ev=1.4846\n",
      "t=131.89s i=4999, l_running=1.1280, tr=1.0477 ev=1.4938\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "t_start = time.time()\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    xb, yb = tr_data_loader.get_batch()\n",
    "    \n",
    "    # Loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % eval_every == 0 or i == num_epochs-1:\n",
    "        train_loss = evaluate(tr_data_loader, eval_iters, device)\n",
    "        eval_loss = evaluate(ev_data_loader, eval_iters, device)\n",
    "        model.train()\n",
    "\n",
    "        t_diff = time.time() - t_start; t_start = time.time()\n",
    "        print(f\"t={t_diff:.2f}s i={i}, l_running={loss.item():.4f}, \"\n",
    "              f\"tr={train_loss:.4f} ev={eval_loss:.4f}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c00d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance log:\n",
    "\n",
    "# t=2.04s i=5000, l_running=2.5973, tr=2.4735 ev=2.4981 - bigram model, pos encodings\n",
    "# t=4.40s i=4999, l_running=2.4050, tr=2.4004 ev=2.3993 - initial self attention\n",
    "# t=10.94s i=4999, l_running=2.2519, tr=2.2574 ev=2.2714 - multi-head self-attention\n",
    "# t=11.33s i=4999, l_running=2.1654, tr=2.2316 ev=2.2393 - projection after multi-head\n",
    "# t=26.44s i=4999, l_running=2.3429, tr=2.2939 ev=2.3086 - deepen to 3x block :S\n",
    "# t=28.57s i=4999, l_running=1.9927, tr=1.9904 ev=2.0723 - residuals and more projections\n",
    "# t=30.31s i=4999, l_running=1.9702, tr=1.9798 ev=2.0575 - add layer-norm\n",
    "# t=18.10s i=4999, l_running=1.9884, tr=1.9837 ev=2.0556 - add minimal dropout :()\n",
    "# t=131.89s i=4999, l_running=1.1280, tr=1.0477 ev=1.4938 - scale up, Karpathy got 1.4873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "97ba3096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Secontent small day or stalger save to charity no for my\n",
      "too must or but and their states.\n",
      "\n",
      "Second Servingman:\n",
      "Then we willing we may prayer of Rome,\n",
      "Or his dangerous accounsel sudding and were need\n",
      "To been hath home war. Go, make comfort, we'll withouts,\n",
      "Didst for natures, to be-laid's prithee,\n",
      "The way is royalth, by the world, whereupon, I would\n",
      "I five, make take vingitor, heavens out\n",
      "should not deliver with power.\n",
      "\n",
      "ISABELLA:\n",
      "This hastest frame her him\n",
      "As he hath Fornce as his none fair but are\n",
      "To nor thee wintwer things, and cannot curse stready.\n",
      "\n",
      "BUCKINGHAM:\n",
      "God shame he is not live.\n",
      "\n",
      "KING RICHman:\n",
      "Let me say is perjoint, he hath been.\n",
      "\n",
      "QUEEN:\n",
      "It is not so more but maid I, see let's brother.\n",
      "O, sir; my unchurching God till to my heart rest:\n",
      "Become for for this servant, and nothing brest,\n",
      "This is the print, and share here sleep to that them?\n",
      "\n",
      "FLORIZEL:\n",
      "Presence, and this swears is friends, afford-hath\n",
      "Of Buckingham too! my lord,\n",
      "All I have know it you fession!\n",
      "The mosder and the more of his itself:\n",
      "O, if it we disprimosition, this news give thee together,\n",
      "Or, my more desire is regreet,\n",
      "Thy Lukent off Henry's from slaint,\n",
      "And make himself our crowned that kneelled;\n",
      "Then we shall not Keepant with worship?\n",
      "\n",
      "PAULINA:\n",
      "I'll ribnament; and for slir title.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "It is know well? my words to remellined.\n",
      "\n",
      "MONTAGUE:\n",
      "What let's I'll not be my lord,\n",
      "The crave maid high, how should beg my valour's down;\n",
      "And is down like her life, grace he hath unclance\n",
      "\n",
      "ISABELLA:\n",
      "Why carry I known would it an dust.\n",
      "\n",
      "GLOUCESTER:\n",
      "Ay, mojey,\n",
      "I wn with did of grief him: now, sir:\n",
      "Corioler judgment his coward? had him your\n",
      "That his hands your tongue yourself; blood stay no close\n",
      "Lady is love man's saids; I do entertain with Her.\n",
      "\n",
      "BUCKINGHAM:\n",
      "All what may some we did, you was yourselves,\n",
      "And swifts thou spitest tnow now, I warrant I?\n",
      "\n",
      "GLOUCESTER:\n",
      "Yet it gage provost the king eye.\n",
      "\n",
      "MARGIUS:\n",
      "Henry you the Volsce to this gentleman:\n",
      "Let must no words, bid intents, do it person my live\n",
      "Forge, good name and hand gone, that me, Pervice she soldier.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "If not so? I have something of the people.\n",
      "\n",
      "KING RICHARD III:\n",
      "Would the sense, devil and you since\n",
      "Of he head in their carse concress'd\n",
      "With himself in dead; when he had you to me,\n",
      "This for the dity thou king, stir? Which resists was away;\n",
      "For thy false, that offection of all\n",
      "To set consent us her day?\n",
      "With think's thou commonded and to thee.\n",
      "Swear the resty of the earth that his: I must I\n",
      "Have set not King Edward Hortens; wh\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    idx = torch.tensor([[0]], device=device)  # B=1, T=1, '\\n'\n",
    "    res = model.generate(idx, n_seq, max_tokens=2500)\n",
    "    print(tok.decode(res[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b9995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfa416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c3feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a509f1bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
