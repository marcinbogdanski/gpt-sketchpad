{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Makemore Part 4: Backprop Ninja</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Manual backprop through a **MLP** model. Pen-and-paper derivations\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "letters = ['.'] + letters\n",
    "n_vocab = len(letters)\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5ce737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63733ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tok, block_size, names):\n",
    "    X, Y = [], []  # inputs and targets\n",
    "    for name in names:\n",
    "        name = '.'*block_size + name + '.'  # add start/stop tokens '..emma.'\n",
    "        for i in range(len(name) - block_size):\n",
    "            X.append(tok.encode(name[i:i+block_size]))\n",
    "            Y.append(tok.encode(name[i+block_size])[0])  # [0] to keep Y 1d tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length\n",
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tok, block_size, names[:n1])\n",
    "Xval, Yval = build_dataset(tok, block_size, names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(tok, block_size, names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af25a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, at, bt):\n",
    "    ex = torch.all(at == bt).item()\n",
    "    app = torch.allclose(at, bt)\n",
    "    maxdiff = (at - bt).abs().max().item()\n",
    "    print(f'{s:18s} | exat: {str(ex):5s} | approx: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52318be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Layers\n",
    "# NOTE: PyTorch F.cross_entropy uses multiple optimizations casuing slight\n",
    "# numerical differences. This \"lucky seed\" makes our manual and PyTorch losses match.\n",
    "# NOTE: instead of 1/x use x**-0.5, PyTorch kernels are not reproducible with 1/x\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "n_batch = 32\n",
    "n_embd = 10\n",
    "n_hid = 64\n",
    "\n",
    "# Model\n",
    "# NOTE: Init all params to non-zero so we don't mask gradient issues\n",
    "C = torch.randn((n_vocab, n_embd))                              # n_vocab, n_emb (embeddings)\n",
    "W1_kaiming_init = (5/3)/((n_embd*block_size)**0.5)              # tanh_gain / sqrt(fan_in)\n",
    "W1 = torch.randn((n_embd*block_size, n_hid)) * W1_kaiming_init  # n_seq*n_emb, n_hid\n",
    "b1 = torch.randn(n_hid)                      * 0.1              # n_hid\n",
    "bngain = torch.randn((1, n_hid))             * 0.1 + 1.0        # 1, n_hid\n",
    "bnbias = torch.randn((1, n_hid))             * 0.1              # 1, n_hid\n",
    "W2 = torch.randn((n_hid, n_vocab))           * 0.1              # n_hid, n_out\n",
    "b2 = torch.randn(n_vocab)                    * 0.1              # 1, n_out\n",
    "\n",
    "# Gather Params\n",
    "params = [C, W1, b1, bngain, bnbias, W2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Random mini batch\n",
    "batch_indices = torch.randint(0, Xtr.shape[0], (n_batch,))\n",
    "x_batch = Xtr[batch_indices]\n",
    "y_batch = Ytr[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc788989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch loss: 3.3894288539886475\n",
      "              3.3894288539886475\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "\n",
    "# Embedding\n",
    "emb = C[x_batch]                                  # n_batch, n_seq, n_emb\n",
    "embcat = emb.view(-1, n_embd*block_size)          # n_batch, n_embd*block_size\n",
    "\n",
    "# Linear 1\n",
    "z1 = embcat @ W1 + b1                             # n_batch, n_hid\n",
    "\n",
    "# Bachnorm 1\n",
    "# zx = (x - x_mean) / (x_var + 1e-5)**0.5\n",
    "# bn(x) = zx * gain + bias\n",
    "# Batchnorm 1 - mean\n",
    "z1_sum = z1.sum(dim=0, keepdim=True)              # 1, n_hid\n",
    "z1_mean = z1_sum / n_batch                        # 1, n_hid\n",
    "# Batchnorm 1 - var\n",
    "z1_diff = z1 - z1_mean                            # n_batch, n_hid\n",
    "z1_diff_2 = z1_diff**2.0                          # n_batch, n_hid\n",
    "z1_diff_sum = z1_diff_2.sum(dim=0, keepdim=True)  # 1, n_hid\n",
    "z1_var = z1_diff_sum * (n_batch-1)**-1            # 1, n_hid\n",
    "# Batchnorm 1 - correction\n",
    "z1_var_p_eps = z1_var + 1e-5                      # 1, n_hid\n",
    "z1_var_sqrt_inv = z1_var_p_eps**-0.5              # 1, n_hid\n",
    "zx = z1_diff * z1_var_sqrt_inv                    # n_batch, n_hid\n",
    "# Batchnorm 1 - scaling\n",
    "zz = bngain * zx + bnbias                         # n_batch, n_hid\n",
    "\n",
    "# Tanh 1\n",
    "# tanh(x) = (torch.exp(2*x) - 1.0) / (torch.exp(2*x) + 1)\n",
    "zz_2 = 2*zz                                 # n_batch, n_hid\n",
    "zz_2_exp = zz_2.exp()                       # n_batch, n_hid\n",
    "zz_2_exp_m1 = zz_2_exp - 1.0                # n_batch, n_hid\n",
    "zz_2_exp_p1 = zz_2_exp + 1.0                # n_batch, n_hid\n",
    "zz_2_exp_p1_inv = zz_2_exp_p1**-1           # n_batch, n_hid\n",
    "h1 = zz_2_exp_m1 * zz_2_exp_p1_inv          # n_batch, n_hid\n",
    "\n",
    "# Linear 2\n",
    "logits = h1 @ W2 + b2                       # n_batch, n_vocab\n",
    "\n",
    "# Softmax\n",
    "logits_max = logits.max(dim=-1, keepdim=True).values      # n_batch, 1\n",
    "logits_diff = logits - logits_max                         # n_batch, n_vocab\n",
    "logits_exp = logits_diff.exp()                            # n_batch, n_vocab\n",
    "logits_exp_sum = logits_exp.sum(dim=-1, keepdim=True)     # n_batch, 1\n",
    "logits_exp_sum_inv = logits_exp_sum**-1                   # n_batch, 1\n",
    "probs = logits_exp * logits_exp_sum_inv                   # n_batch, n_vocab\n",
    "\n",
    "# Cross Entropy\n",
    "log_probs = probs.log()                                         # n_batch, n_vocab\n",
    "log_probs_target = log_probs[range(n_batch), y_batch]           # n_batch\n",
    "loss = -log_probs_target.sum(dim=0, keepdim=True) / n_batch\n",
    "\n",
    "# PyTorch Backward Pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "        emb, embcat,\n",
    "        z1,\n",
    "        z1_sum, z1_mean,\n",
    "        z1_diff, z1_diff_2, z1_diff_sum, z1_var,\n",
    "        z1_var_p_eps, z1_var_sqrt_inv, zx,\n",
    "        zz,\n",
    "        zz_2, zz_2_exp, zz_2_exp_m1, zz_2_exp_p1, zz_2_exp_p1_inv, h1,\n",
    "        logits,\n",
    "        logits_max, logits_diff, logits_exp, logits_exp_sum, logits_exp_sum_inv, probs,\n",
    "        log_probs, log_probs_target, loss\n",
    "    ]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # PyTorch loss: 3.3894288539886475\n",
    "    print(f\"PyTorch loss: {F.cross_entropy(logits, y_batch).item()}\")\n",
    "    print(f\"              {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da356ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # Cross Entropy\n",
    "    d_loss = torch.tensor(1.0)\n",
    "    d_log_probs_target = -1 * torch.ones(n_batch) * (1/n_batch * d_loss)\n",
    "    d_log_probs = torch.zeros_like(log_probs)\n",
    "    d_log_probs[range(n_batch), y_batch] = d_log_probs_target\n",
    "\n",
    "    # Softmax\n",
    "    d_probs = (1 / probs) * d_log_probs    # deriv of ln * chain rule\n",
    "    d_logits_exp_sum_inv = (logits_exp * d_probs).sum(dim=-1, keepdim=True)\n",
    "    d_logits_exp_sum = -1 * logits_exp_sum**-2 * d_logits_exp_sum_inv\n",
    "    d_logits_exp = torch.ones_like(logits_exp) * d_logits_exp_sum\n",
    "    d_logits_exp += logits_exp_sum_inv * d_probs  # second gradient path\n",
    "    d_logits_diff = logits_exp * d_logits_exp\n",
    "    d_logits_max = -d_logits_diff.sum(dim=-1, keepdim=True)   # n_batch, 1\n",
    "    d_logits = d_logits_diff.clone()  # logits_diff path\n",
    "    logits_maxi = logits.max(dim=-1).indices\n",
    "    d_logits[range(n_batch), logits_maxi] += d_logits_max[range(n_batch), 0]   # max path\n",
    "\n",
    "    # Linear 2\n",
    "    d_h1 = d_logits @ W2.T\n",
    "    d_W2 = h1.T @ d_logits\n",
    "    d_b2 = d_logits.sum(dim=0)\n",
    "\n",
    "    # Tanh 1\n",
    "    d_zz_2_exp_p1_inv = zz_2_exp_m1 * d_h1\n",
    "    d_zz_2_exp_p1 = -1 * zz_2_exp_p1**-2 * d_zz_2_exp_p1_inv\n",
    "    d_zz_2_exp_m1 = zz_2_exp_p1_inv * d_h1\n",
    "    d_zz_2_exp = d_zz_2_exp_p1 + d_zz_2_exp_m1\n",
    "    d_zz_2 = zz_2_exp * d_zz_2_exp\n",
    "    d_zz = 2 * d_zz_2\n",
    "\n",
    "    # Batch Norm 1 - scaling\n",
    "    d_bngain = (zx * d_zz).sum(dim=0, keepdim=True)\n",
    "    d_bnbias = d_zz.sum(dim=0, keepdim=True)\n",
    "    # Batch Norm 1 - correction\n",
    "    d_zx = torch.ones_like(zx) * bngain * d_zz\n",
    "    d_z1_var_sqrt_inv = (z1_diff * d_zx).sum(dim=0, keepdim=True)\n",
    "    d_z1_var_p_eps = -0.5*z1_var_p_eps**-1.5 * d_z1_var_sqrt_inv\n",
    "    # Batch Norm 1 - var\n",
    "    d_z1_var = d_z1_var_p_eps.clone()\n",
    "    d_z1_diff_sum = (n_batch-1)**-1 * d_z1_var\n",
    "    d_z1_diff_2 = torch.ones_like(z1_diff_2) * d_z1_diff_sum\n",
    "    d_z1_diff = 2 * z1_diff * d_z1_diff_2\n",
    "    d_z1_diff += d_zx * z1_var_sqrt_inv  # second path\n",
    "    # Batch Norm 1 - mean\n",
    "    d_z1_mean = -1 * d_z1_diff.sum(dim=0, keepdim=True)\n",
    "    d_z1_sum = 1/n_batch * d_z1_mean\n",
    "    d_z1 = torch.ones_like(z1) * d_z1_sum\n",
    "    d_z1 += d_z1_diff\n",
    "\n",
    "    # Linear 1\n",
    "    d_embcat = d_z1 @ W1.T\n",
    "    d_W1 = embcat.T @ d_z1\n",
    "    d_b1 = d_z1.sum(dim=0)\n",
    "\n",
    "    # Embedding\n",
    "    d_emb = d_embcat.view(-1, block_size, n_embd)\n",
    "    d_C = torch.zeros_like(C)\n",
    "    d_C.index_add_(0, x_batch.view(-1), d_emb.view(-1, n_embd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9196d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss               | exat: True  | approx: True  | maxdiff: 0.0\n",
      "log_probs_target   | exat: True  | approx: True  | maxdiff: 0.0\n",
      "log_probs          | exat: True  | approx: True  | maxdiff: 0.0\n",
      "probs              | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp_sum_inv | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp_sum     | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp         | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_diff        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_max         | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "h1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "W2                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "b2                 | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy\n",
    "cmp(\"loss\", d_loss, loss.grad)\n",
    "cmp(\"log_probs_target\", d_log_probs_target, log_probs_target.grad)\n",
    "cmp(\"log_probs\", d_log_probs, log_probs.grad)\n",
    "\n",
    "# Softmax\n",
    "cmp(\"probs\", d_probs, probs.grad)\n",
    "cmp(\"logits_exp_sum_inv\", d_logits_exp_sum_inv, logits_exp_sum_inv.grad)\n",
    "cmp(\"logits_exp_sum\", d_logits_exp_sum, logits_exp_sum.grad)\n",
    "cmp(\"logits_exp\", d_logits_exp, logits_exp.grad)\n",
    "cmp(\"logits_diff\", d_logits_diff, logits_diff.grad)\n",
    "cmp(\"logits_max\", d_logits_max, logits_max.grad)\n",
    "cmp(\"logits\", d_logits, logits.grad)\n",
    "\n",
    "# Linear 2\n",
    "cmp(\"h1\", d_h1, h1.grad)\n",
    "cmp(\"W2\", d_W2, W2.grad)\n",
    "cmp(\"b2\", d_b2, b2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6771196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zz_2_exp_p1_inv    | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp_p1        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp_m1        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp           | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2               | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "bngain             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "bnbias             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zx                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var_sqrt_inv    | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var_p_eps       | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff_sum        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff_2          | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff            | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_mean            | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_sum             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "embcat             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "W1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "b1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "emb                | exat: True  | approx: True  | maxdiff: 0.0\n",
      "C                  | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Tanh 1\n",
    "cmp(\"zz_2_exp_p1_inv\", d_zz_2_exp_p1_inv, zz_2_exp_p1_inv.grad)\n",
    "cmp(\"zz_2_exp_p1\", d_zz_2_exp_p1, zz_2_exp_p1.grad)\n",
    "cmp(\"zz_2_exp_m1\", d_zz_2_exp_m1, zz_2_exp_m1.grad)\n",
    "cmp(\"zz_2_exp\", d_zz_2_exp, zz_2_exp.grad)\n",
    "cmp(\"zz_2\", d_zz_2, zz_2.grad)\n",
    "\n",
    "# Batch Norm 1 - scaling\n",
    "cmp(\"zz\", d_zz, zz.grad)\n",
    "cmp(\"bngain\", d_bngain, bngain.grad)\n",
    "cmp(\"bnbias\", d_bnbias, bnbias.grad)\n",
    "# Batch Norm 1 - correction\n",
    "cmp(\"zx\", d_zx, zx.grad)\n",
    "cmp(\"z1_var_sqrt_inv\", d_z1_var_sqrt_inv, z1_var_sqrt_inv.grad)\n",
    "cmp(\"z1_var_p_eps\", d_z1_var_p_eps, z1_var_p_eps.grad)\n",
    "# Batch Norm 1 - var\n",
    "cmp(\"z1_var\", d_z1_var, z1_var.grad)\n",
    "cmp(\"z1_diff_sum\", d_z1_diff_sum, z1_diff_sum.grad)\n",
    "cmp(\"z1_diff_2\", d_z1_diff_2, z1_diff_2.grad)\n",
    "cmp(\"z1_diff\", d_z1_diff, z1_diff.grad)\n",
    "# Batch Norm 1 - mean\n",
    "cmp(\"z1_mean\", d_z1_mean, z1_mean.grad)\n",
    "cmp(\"z1_sum\", d_z1_sum, z1_sum.grad)\n",
    "cmp(\"z1\", d_z1, z1.grad)\n",
    "\n",
    "# Linear 1\n",
    "cmp(\"embcat\", d_embcat, embcat.grad)\n",
    "cmp(\"W1\", d_W1, W1.grad)\n",
    "cmp(\"b1\", d_b1, b1.grad)\n",
    "\n",
    "# Embedding\n",
    "cmp(\"emb\", d_emb, emb.grad)\n",
    "cmp(\"C\", d_C, C.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da550b3",
   "metadata": {},
   "source": [
    "# Cross Entropy - derive better graddient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad473204",
   "metadata": {},
   "source": [
    "Loss:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "Softmax:\n",
    "$$\n",
    "p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Gradient\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} = \\mathbf{p} - \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f7bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_xxx           | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Forwawrd - before\n",
    "\n",
    "# # Softmax\n",
    "# logits_max = logits.max(dim=-1, keepdim=True).values      # n_batch, 1\n",
    "# logits_diff = logits - logits_max                         # n_batch, n_vocab\n",
    "# logits_exp = logits_diff.exp()                            # n_batch, n_vocab\n",
    "# logits_exp_sum = logits_exp.sum(dim=-1, keepdim=True)     # n_batch, 1\n",
    "# logits_exp_sum_inv = logits_exp_sum**-1                   # n_batch, 1\n",
    "# probs = logits_exp * logits_exp_sum_inv                   # n_batch, n_vocab\n",
    "\n",
    "# # Cross Entropy\n",
    "# log_probs = probs.log()                                         # n_batch, n_vocab\n",
    "# log_probs_target = log_probs[range(n_batch), y_batch]           # n_batch\n",
    "# loss = -log_probs_target.sum(dim=0, keepdim=True) / n_batch\n",
    "\n",
    "# Forward - new\n",
    "with torch.no_grad():\n",
    "    loss_xxx = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "cmp(\"loss_xxx\", loss_xxx, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46df7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits             | exat: False | approx: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward - before\n",
    "\n",
    "# # Cross Entropy\n",
    "# d_loss = torch.tensor(1.0)\n",
    "# d_log_probs_target = -1 * torch.ones(n_batch) * (1/n_batch * d_loss)\n",
    "# d_log_probs = torch.zeros_like(log_probs)\n",
    "# d_log_probs[range(n_batch), y_batch] = d_log_probs_target\n",
    "\n",
    "# # Softmax\n",
    "# d_probs = (1 / probs) * d_log_probs    # deriv of ln * chain rule\n",
    "# d_logits_exp_sum_inv = (logits_exp * d_probs).sum(dim=-1, keepdim=True)\n",
    "# d_logits_exp_sum = -1 * logits_exp_sum**-2 * d_logits_exp_sum_inv\n",
    "# d_logits_exp = torch.ones_like(logits_exp) * d_logits_exp_sum\n",
    "# d_logits_exp += logits_exp_sum_inv * d_probs  # second gradient path\n",
    "# d_logits_diff = logits_exp * d_logits_exp\n",
    "# d_logits_max = -d_logits_diff.sum(dim=-1, keepdim=True)   # n_batch, 1\n",
    "# d_logits = d_logits_diff.clone()  # logits_diff path\n",
    "# logits_maxi = logits.max(dim=-1).indices\n",
    "# d_logits[range(n_batch), logits_maxi] += d_logits_max[range(n_batch), 0]   # max path\n",
    "\n",
    "# Backward - new\n",
    "with torch.no_grad():\n",
    "    d_logits_xxx = F.softmax(logits, dim=1)\n",
    "    d_logits_xxx[range(n_batch), y_batch] -= 1\n",
    "    d_logits_xxx /= n_batch\n",
    "\n",
    "cmp(\"logits\", d_logits_xxx, logits.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee64c3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d_logits_xxx.detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5151ff",
   "metadata": {},
   "source": [
    "# Tanh - derive better graddient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d827b85",
   "metadata": {},
   "source": [
    "Tanh\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tanh(x)}{\\partial x} = 1 - \\tanh^2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d68b162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1_xxx             | exat: False | approx: True  | maxdiff: 1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "# Forward - old\n",
    "\n",
    "# # Tanh 1\n",
    "# # tanh(x) = (torch.exp(2*x) - 1.0) / (torch.exp(2*x) + 1)\n",
    "# zz_2 = 2*zz                                 # n_batch, n_hid\n",
    "# zz_2_exp = zz_2.exp()                       # n_batch, n_hid\n",
    "# zz_2_exp_m1 = zz_2_exp - 1.0                # n_batch, n_hid\n",
    "# zz_2_exp_p1 = zz_2_exp + 1.0                # n_batch, n_hid\n",
    "# zz_2_exp_p1_inv = zz_2_exp_p1**-1           # n_batch, n_hid\n",
    "# h1 = zz_2_exp_m1 * zz_2_exp_p1_inv          # n_batch, n_hid\n",
    "\n",
    "# Forward - new\n",
    "with torch.no_grad():\n",
    "    h1_xxx = torch.tanh(zz)\n",
    "\n",
    "cmp(\"h1_xxx\", h1_xxx, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_zz_xxx           | exat: False | approx: True  | maxdiff: 1.6298145055770874e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward - old\n",
    "\n",
    "# # Tanh 1\n",
    "# d_zz_2_exp_p1_inv = zz_2_exp_m1 * d_h1\n",
    "# d_zz_2_exp_p1 = -1 * zz_2_exp_p1**-2 * d_zz_2_exp_p1_inv\n",
    "# d_zz_2_exp_m1 = zz_2_exp_p1_inv * d_h1\n",
    "# d_zz_2_exp = d_zz_2_exp_p1 + d_zz_2_exp_m1\n",
    "# d_zz_2 = zz_2_exp * d_zz_2_exp\n",
    "# d_zz = 2 * d_zz_2\n",
    "\n",
    "# Backward - new\n",
    "with torch.no_grad():\n",
    "    d_zz_xxx = (1 - torch.tanh(zz)**2)  *   d_h1  # don't forget chain rule\n",
    "cmp(\"d_zz_xxx\", d_zz_xxx, zz.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd697297",
   "metadata": {},
   "source": [
    "# Batch Norm - derive better graddient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea321a0",
   "metadata": {},
   "source": [
    "Mean:\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum x_i\n",
    "$$\n",
    "\n",
    "Variance:\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{m-1} \\sum (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Normalized value:\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Scale and shift:\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963dda93",
   "metadata": {},
   "source": [
    "Computation Flow:\n",
    "- top: path through the $\\mu$\n",
    "- mid: path through the $\\sigma^2$, note gradient $\\sigma^2\\rightarrow\\mu$ is zero, hence crossed arrow\n",
    "- bottom: direct path\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\vdots \\\\ x \\\\ \\vdots \\end{bmatrix}\n",
    "\\begin{array}{ccc}\n",
    " \\rightarrow & \\mu & \\rightarrow \\\\\n",
    " & \\downarrow \\!\\!\\! \\times & \\\\\n",
    " \\rightarrow & \\sigma^2 & \\rightarrow \\\\\n",
    " & & \\\\\n",
    " & \\longrightarrow &\n",
    "\\end{array}\n",
    "\\begin{bmatrix} \\vdots \\\\ \\hat{x} \\\\ \\vdots \\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix} \\vdots \\\\ y \\\\ \\vdots \\end{bmatrix}\n",
    "\\rightarrow\n",
    "L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4219bc",
   "metadata": {},
   "source": [
    "Our target:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} \\cdot \\frac{d\\hat{x}_j}{dx_i}\n",
    "$$\n",
    "\n",
    "The first term $\\frac{\\partial L}{\\partial \\hat{x}_j}$ where $\\frac{\\partial L}{\\partial y_j}$ is upstream gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{x}_j} = \\frac{\\partial L}{\\partial y_j} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "Now the $\\frac{d\\hat{x}_j}{dx_i}$ term, note three paths: direct, through $\\mu$ and $\\sigma^2$:\n",
    "$$\n",
    "\\frac{d\\hat{x}_j}{dx_i} = \\frac{\\partial \\hat{x}_j}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} \\quad \\text{(note: } \\frac{\\partial \\hat{x}_j}{\\partial x_i} = 0 \\text{ when } j \\neq i \\text{)}\n",
    "$$\n",
    "\n",
    "Now one by one:\n",
    "\n",
    "Partial $x\\rightarrow\\hat{x}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left[ (x_j - \\mu)(\\sigma^2 + \\epsilon)^{-1/2} \\right] = \\begin{cases} (\\sigma^2 + \\epsilon)^{-1/2} & \\text{if } j = i \\\\ 0 & \\text{if } j \\neq i \\end{cases}\n",
    "$$\n",
    "\n",
    "Partial $\\sigma^2 \\rightarrow \\hat{x}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left[ (x_j - \\mu)(\\sigma^2 + \\epsilon)^{-1/2} \\right] = (x_j - \\mu) \\cdot \\left( -\\frac{1}{2} \\right) \\cdot (\\sigma^2 + \\epsilon)^{-3/2}\n",
    "$$\n",
    "\n",
    "Partial $\\mu \\rightarrow \\sigma^2$ In last step we substitute from $m\\mu=\\sum x_i$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma^2}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ \\frac{1}{m-1} \\sum (x_i - \\mu)^2 \\right] = \\frac{2}{m-1} \\left( \\sum x_i - \\sum \\mu \\right) =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{2}{m-1} \\left( \\sum x_i - m\\mu \\right) = \\frac{2}{m-1} \\left( \\sum x_i - \\sum x_i \\right) = 0\n",
    "$$\n",
    "\n",
    "Partial $\\mu\\rightarrow\\hat{x}_j$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}_j}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ (x_j - \\mu)(\\sigma^2 + \\epsilon)^{-1/2} \\right] = -(\\sigma^2 + \\epsilon)^{-1/2}\n",
    "$$\n",
    "\n",
    "Total $\\mu\\rightarrow\\hat{x}$, note $\\frac{\\partial \\sigma^2}{\\partial \\mu}=0$ so second term dissappears\n",
    "\n",
    "$$\n",
    "\\frac{d\\hat{x}_j}{d\\mu} = \\frac{\\partial \\hat{x}_j}{\\partial \\mu} + \\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial \\mu} = -(\\sigma^2 + \\epsilon)^{-1/2}\n",
    "$$\n",
    "\n",
    "Partial $x_i\\rightarrow\\mu$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mu}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left[ \\frac{1}{m} \\sum_j x_j \\right] = \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "Partial $x_i\\rightarrow\\sigma^2$\n",
    "$$\n",
    "\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left[ \\frac{1}{m-1} \\sum_j (x_j - \\mu)^2 \\right] = \\frac{2}{m-1} (x_i - \\mu)\n",
    "$$\n",
    "\n",
    "Let's combine all of them\n",
    "\n",
    "$$\n",
    "\\frac{d\\hat{x}_j}{dx_i} = \\frac{\\partial \\hat{x}_j}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial \\hat{x}_j}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\sigma^2 + \\epsilon)^{-1/2} \\bigg|_{i=j} + \\left( -\\frac{1}{m} (\\sigma^2 + \\epsilon)^{-1/2} \\right) + (x_j - \\mu) \\left( -\\frac{1}{2} \\right) (\\sigma^2 + \\epsilon)^{-3/2} \\left( \\frac{2}{m-1} \\right) (x_i - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (\\sigma^2 + \\epsilon)^{-1/2} \\bigg|_{i=j} + \\left( -\\frac{1}{m} (\\sigma^2 + \\epsilon)^{-1/2} \\right) + (\\sigma^2 + \\epsilon)^{-1/2} \\left( \\frac{x_j - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\right) \\left( -\\frac{1}{m-1} \\right) \\left( \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(\\sigma^2 + \\epsilon)^{-1/2}}{m} \\left[ m \\big|_{i=j} - 1 - \\frac{m}{m-1} \\hat{x}_i \\hat{x}_j \\right]\n",
    "$$\n",
    "\n",
    "Back to the task at hand\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} \\cdot \\frac{d\\hat{x}_j}{dx_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{x}_j} = \\frac{\\partial L}{\\partial y_j} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\sum_j \\left[ \\gamma \\cdot \\frac{\\partial L}{\\partial y_j} \\right] \\cdot \\frac{(\\sigma^2 + \\epsilon)^{-1/2}}{m} \\left[ m \\big|_{i=j} - 1 - \\frac{m}{m-1} \\hat{x}_i \\hat{x}_j \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\gamma \\frac{(\\sigma^2 + \\epsilon)^{-1/2}}{m} \\left[ m \\cdot \\frac{\\partial L}{\\partial y_i} - \\sum_j \\frac{\\partial L}{\\partial y_j} - \\frac{m}{m-1} \\sum_j \\frac{\\partial L}{\\partial y_j} \\cdot \\hat{x}_i \\hat{x}_j \\right]\n",
    "$$\n",
    "\n",
    "Which is exactly what [Karpathy got in his video](https://youtu.be/q8SA3rM6ckI?si=uoYTpPWqNpqcKImp&t=6381) :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf70f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff:  tensor(4.7684e-07)\n"
     ]
    }
   ],
   "source": [
    "# Forward - old\n",
    "\n",
    "# # Bachnorm 1\n",
    "# # zx = (x - x_mean) / (x_var + 1e-5)**0.5\n",
    "# # bn(x) = zx * gain + bias\n",
    "# # Batchnorm 1 - mean\n",
    "# z1_sum = z1.sum(dim=0, keepdim=True)              # 1, n_hid\n",
    "# z1_mean = z1_sum / n_batch                        # 1, n_hid\n",
    "# # Batchnorm 1 - var\n",
    "# z1_diff = z1 - z1_mean                            # n_batch, n_hid\n",
    "# z1_diff_2 = z1_diff**2.0                          # n_batch, n_hid\n",
    "# z1_diff_sum = z1_diff_2.sum(dim=0, keepdim=True)  # 1, n_hid\n",
    "# z1_var = z1_diff_sum * (n_batch-1)**-1            # 1, n_hid\n",
    "# # Batchnorm 1 - correction\n",
    "# z1_var_p_eps = z1_var + 1e-5                      # 1, n_hid\n",
    "# z1_var_sqrt_inv = z1_var_p_eps**-0.5              # 1, n_hid\n",
    "# zx = z1_diff * z1_var_sqrt_inv                    # n_batch, n_hid\n",
    "# # Batchnorm 1 - scaling\n",
    "# zz = bngain * zx + bnbias                         # n_batch, n_hid\n",
    "\n",
    "# Forward - new\n",
    "with torch.no_grad():\n",
    "    zz_xxx = bngain * (z1 - z1.mean(0, keepdim=True)) / torch.sqrt(z1.var(0, keepdim=True, unbiased=True)+1e-5) + bnbias\n",
    "\n",
    "    print(\"max diff: \", (zz_xxx - zz).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7975bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1                 | exat: False | approx: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# Backward - old\n",
    "# # Batch Norm 1 - scaling\n",
    "# d_bngain = (zx * d_zz).sum(dim=0, keepdim=True)\n",
    "# d_bnbias = d_zz.sum(dim=0, keepdim=True)\n",
    "# # Batch Norm 1 - correction\n",
    "# d_zx = torch.ones_like(zx) * bngain * d_zz\n",
    "# d_z1_var_sqrt_inv = (z1_diff * d_zx).sum(dim=0, keepdim=True)\n",
    "# d_z1_var_p_eps = -0.5*z1_var_p_eps**-1.5 * d_z1_var_sqrt_inv\n",
    "# # Batch Norm 1 - var\n",
    "# d_z1_var = d_z1_var_p_eps.clone()\n",
    "# d_z1_diff_sum = (n_batch-1)**-1 * d_z1_var\n",
    "# d_z1_diff_2 = torch.ones_like(z1_diff_2) * d_z1_diff_sum\n",
    "# d_z1_diff = 2 * z1_diff * d_z1_diff_2\n",
    "# d_z1_diff += d_zx * z1_var_sqrt_inv  # second path\n",
    "# # Batch Norm 1 - mean\n",
    "# d_z1_mean = -1 * d_z1_diff.sum(dim=0, keepdim=True)\n",
    "# d_z1_sum = 1/n_batch * d_z1_mean\n",
    "# d_z1 = torch.ones_like(z1) * d_z1_sum\n",
    "# d_z1 += d_z1_diff\n",
    "\n",
    "# Backward - new\n",
    "with torch.no_grad():\n",
    "    d_z1_xxx = bngain * z1_var_sqrt_inv / n_batch * (\n",
    "        n_batch * d_zz \n",
    "        - d_zz.sum(0) \n",
    "        - n_batch/(n_batch-1) * zx * (d_zz * zx).sum(0)\n",
    "    )\n",
    "\n",
    "cmp(\"z1\", d_z1_xxx, z1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7307e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a77ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
