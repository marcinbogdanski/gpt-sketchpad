{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa83d77",
   "metadata": {},
   "source": [
    "# Efficient Multi-Head Attention\n",
    "\n",
    "Also exercise in understanding operand dimensionality in batched mat-muls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979f8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d738bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4     # num independent examples\n",
    "block_size = 1024  # max sequence length\n",
    "n_embd = 768       # total embedding dim, both in and out, divisible by n_head\n",
    "n_head = 12        # number of heads\n",
    "\n",
    "# Init\n",
    "torch.manual_seed(42)\n",
    "c_attn_W = torch.randn(2304, 768) / 2304**0.5\n",
    "c_attn_b = torch.randn(2304)\n",
    "c_proj_W = torch.randn(768, 768) / 768**0.5\n",
    "c_proj_b = torch.randn(768)\n",
    "x = torch.randn(batch_size,block_size,n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae147f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference Implementation (unoptimized, Karpathy video)\n",
    "\n",
    "class GPTConfig:\n",
    "    def __init__(self, block_size, n_embd, n_head):\n",
    "        self.block_size: int = block_size  # max sequence length\n",
    "        self.n_head: int = n_head  # number of heads\n",
    "        self.n_embd: int = n_embd # embedding dimension\n",
    "\n",
    "class CausalSelfAttentionKarpathy1(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        # attention (materializes the large (t,T) matrix for all the queries and keys)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, T)\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4b2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias torch.Size([1, 1, 1024, 1024])\n",
      "c_attn.weight torch.Size([2304, 768])\n",
      "c_attn.bias torch.Size([2304])\n",
      "c_proj.weight torch.Size([768, 768])\n",
      "c_proj.bias torch.Size([768])\n",
      "--- dst ---\n",
      "torch.Size([4, 1024, 768])\n",
      "-18783.59375\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig(block_size=block_size, n_embd=n_embd, n_head=n_head)\n",
    "csa_k1 = CausalSelfAttentionKarpathy1(config)\n",
    "\n",
    "# Load Weights\n",
    "csa_k1_state = csa_k1.state_dict()\n",
    "for k, v in csa_k1_state.items():\n",
    "    print(k, v.shape)\n",
    "print('--- dst ---')\n",
    "csa_k1_state['c_attn.weight'] = c_attn_W.clone()\n",
    "csa_k1_state['c_attn.bias'] = c_attn_b.clone()\n",
    "csa_k1_state['c_proj.weight'] = c_proj_W.clone()\n",
    "csa_k1_state['c_proj.bias'] = c_proj_b.clone()\n",
    "csa_k1.load_state_dict(csa_k1_state)\n",
    "\n",
    "# Run\n",
    "y_k1 = csa_k1(x)\n",
    "print(y_k1.shape)\n",
    "print(y_k1.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af2a591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- src ---\n",
      "in_proj_weight torch.Size([2304, 768])\n",
      "in_proj_bias torch.Size([2304])\n",
      "out_proj.weight torch.Size([768, 768])\n",
      "out_proj.bias torch.Size([768])\n",
      "torch.Size([4, 1024, 768])\n",
      "-18783.6015625\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "csa_pt = torch.nn.MultiheadAttention(\n",
    "    embed_dim=n_embd,\n",
    "    num_heads=n_head,\n",
    "    dropout=0.0,\n",
    "    bias=True,\n",
    "    add_bias_kv=False,\n",
    "    add_zero_attn=False,\n",
    "    kdim=None,\n",
    "    vdim=None,\n",
    "    batch_first=True,  # !\n",
    "    device=None,\n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "# Copy Weights\n",
    "print('--- src ---')\n",
    "csa_pt_state = csa_pt.state_dict()\n",
    "for k,v in csa_pt_state.items():\n",
    "    print(k, v.shape)\n",
    "csa_pt_state['in_proj_weight'] = c_attn_W.clone()\n",
    "csa_pt_state['in_proj_bias'] = c_attn_b.clone()\n",
    "csa_pt_state['out_proj.weight'] = c_proj_W.clone()\n",
    "csa_pt_state['out_proj.bias'] = c_proj_b.clone()\n",
    "csa_pt.load_state_dict(csa_pt_state)\n",
    "\n",
    "# Run\n",
    "attn_mask = torch.nn.Transformer.generate_square_subsequent_mask(block_size)\n",
    "y_pt = csa_pt(x, x, x, attn_mask=attn_mask, is_causal=True)[0]   # (output, attention_weights)\n",
    "print(y_pt.shape)\n",
    "print(y_pt.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c97a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Implementaiton - loopy\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One self-attention head\"\"\"\n",
    "    def __init__(self, block_size, n_embd, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_W = torch.randn(head_size, n_embd)\n",
    "        self.query_b = torch.randn(head_size)\n",
    "        self.key_W = torch.randn(head_size, n_embd)\n",
    "        self.key_b = torch.randn(head_size)\n",
    "        self.value_W = torch.randn(head_size, n_embd)\n",
    "        self.value_b = torch.randn(head_size)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,Ch = x.shape\n",
    "\n",
    "        x_query = x @ self.query_W.T + self.query_b  # B,T,H\n",
    "        x_key = x @ self.key_W.T + self.key_b  # B,T,H\n",
    "        x_value = x @ self.value_W.T + self.value_b  # B,T,H\n",
    "\n",
    "        H = x_key.shape[-1]\n",
    "        W_affin = x_query @ x_key.mT / H**0.5 # / H**0.5  # B,T,T <- B,T,C @ B,C,T\n",
    "        W_affin = W_affin.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        W_affin = torch.softmax(W_affin, dim=-1)\n",
    "\n",
    "        out = W_affin @ x_value\n",
    "        return out\n",
    "    \n",
    "class MultiHead(nn.Module):\n",
    "    \"\"\"Multiple self-attention heads\"\"\"\n",
    "    def __init__(self, block_size, n_head, n_embd):\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = n_embd // n_head\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(block_size, n_embd, head_size) for _ in range(n_head)]\n",
    "        )\n",
    "\n",
    "        self.proj_W = torch.randn(n_embd, n_embd)\n",
    "        self.proj_b = torch.randn(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = x @ self.proj_W.T + self.proj_b\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb2ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n",
      "-18783.595703125\n"
     ]
    }
   ],
   "source": [
    "csa_m1 = MultiHead(block_size=block_size, n_head=n_head, n_embd=n_embd)\n",
    "\n",
    "q_W, k_W, v_W = c_attn_W.split(n_embd, dim=0)\n",
    "q_b, k_b, v_b = c_attn_b.split(n_embd, dim=0)\n",
    "\n",
    "q_W_multi = q_W.split(n_embd//n_head, dim=0)\n",
    "q_b_multi = q_b.split(n_embd//n_head, dim=0)\n",
    "k_W_multi = k_W.split(n_embd//n_head, dim=0)\n",
    "k_b_multi = k_b.split(n_embd//n_head, dim=0)\n",
    "v_W_multi = v_W.split(n_embd//n_head, dim=0)\n",
    "v_b_multi = v_b.split(n_embd//n_head, dim=0)\n",
    "\n",
    "for i, h in enumerate(csa_m1.heads):\n",
    "    h.query_W = q_W_multi[i].clone()\n",
    "    h.query_b = q_b_multi[i].clone()\n",
    "    h.key_W = k_W_multi[i].clone()\n",
    "    h.key_b = k_b_multi[i].clone()\n",
    "    h.value_W = v_W_multi[i].clone()\n",
    "    h.value_b = v_b_multi[i].clone()\n",
    "csa_m1.proj_W = c_proj_W.clone()\n",
    "csa_m1.proj_b = c_proj_b.clone()\n",
    "\n",
    "y_m1 = csa_m1(x)\n",
    "print(y_m1.shape)\n",
    "print(y_m1.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772efc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My implementation - efficient BLAS\n",
    "\n",
    "class CausalSelfAttentionMarcin2(nn.Module):\n",
    "    \"\"\"Multiple self-attention heads\"\"\"\n",
    "    def __init__(self, block_size, n_head, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3*n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones((1, 1, block_size, block_size))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(n_embd, dim=2)  # B, T, nh*hs\n",
    "        q = q.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        k = k.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        v = v.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        q = q.transpose(1, 2)  # B,nh,T,hs\n",
    "        k = k.transpose(1, 2)  # B,nh,T,hs\n",
    "        v = v.transpose(1, 2)  # B,nh,T,hs\n",
    "\n",
    "        H = k.shape[-1]\n",
    "        W_affin = q @ k.mT / H**0.5  # B,nh,T,hs @ B,nh,hs,T -> B,nh,T,T\n",
    "        W_affin = W_affin.masked_fill(self.bias[:,:,:T,:T]==0, float('-inf'))\n",
    "        W_affin = torch.softmax(W_affin, dim=-1)  # B,nh,T,T\n",
    "        raw = W_affin @ v    # B,nh,T,T @ B,nh,T,hs -> B,nh,T,hs\n",
    "\n",
    "        y = raw.transpose(1, 2)  # B,T,nh,hs\n",
    "        y = y.contiguous()\n",
    "        y = y.view(B,T,n_embd)\n",
    "\n",
    "        out = self.c_proj(y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b52b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n",
      "-18783.59375\n"
     ]
    }
   ],
   "source": [
    "csa_m2 = CausalSelfAttentionMarcin2(block_size=block_size, n_head=n_head, n_embd=n_embd)\n",
    "csa_m2_state = csa_m2.state_dict()\n",
    "csa_m2_state['c_attn.weight'] = c_attn_W.clone()\n",
    "csa_m2_state['c_attn.bias'] = c_attn_b.clone()\n",
    "csa_m2_state['c_proj.weight'] = c_proj_W.clone()\n",
    "csa_m2_state['c_proj.bias'] = c_proj_b.clone()\n",
    "csa_m2.load_state_dict(csa_m2_state)\n",
    "\n",
    "y_m2 = csa_m2(x)\n",
    "print(y_m2.shape)\n",
    "print(y_m2.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My implementation - fused fast attention\n",
    "\n",
    "class CausalSelfAttentionMarcin2(nn.Module):\n",
    "    \"\"\"Multiple self-attention heads\"\"\"\n",
    "    def __init__(self, block_size, n_head, n_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3*n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones((1, 1, block_size, block_size))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(n_embd, dim=2)  # B, T, nh*hs\n",
    "        q = q.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        k = k.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        v = v.view(B, T, self.n_head, C//self.n_head)  # B,T,nh,hs\n",
    "        q = q.transpose(1, 2)  # B,nh,T,hs\n",
    "        k = k.transpose(1, 2)  # B,nh,T,hs\n",
    "        v = v.transpose(1, 2)  # B,nh,T,hs\n",
    "\n",
    "        H = k.shape[-1]\n",
    "        W_affin = q @ k.mT / H**0.5  # B,nh,T,hs @ B,nh,hs,T -> B,nh,T,T\n",
    "        W_affin = W_affin.masked_fill(self.bias[:,:,:T,:T]==0, float('-inf'))\n",
    "        W_affin = torch.softmax(W_affin, dim=-1)  # B,nh,T,T\n",
    "        raw = W_affin @ v    # B,nh,T,T @ B,nh,T,hs -> B,nh,T,hs\n",
    "\n",
    "        y = raw.transpose(1, 2)  # B,T,nh,hs\n",
    "        y = y.contiguous()\n",
    "        y = y.view(B,T,n_embd)\n",
    "\n",
    "        out = self.c_proj(y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20665769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c8b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([768, 768])\n",
      "bias torch.Size([768])\n",
      "tensor([[[ 3.9189, -0.4557, -0.6260],\n",
      "         [-0.3068, -1.9111, -0.4942],\n",
      "         [ 1.1698, -2.4789,  0.7141]],\n",
      "\n",
      "        [[ 2.0960, -2.3606, -1.1278],\n",
      "         [ 1.2358, -2.2632, -0.9145],\n",
      "         [ 1.7210, -1.1523,  0.2450]],\n",
      "\n",
      "        [[ 1.8260, -3.1186, -0.2035],\n",
      "         [ 1.1445, -0.6755,  0.3767],\n",
      "         [ 1.1435, -1.0695, -1.0294]]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[ 3.9189, -0.4557, -0.6260],\n",
      "         [-0.3068, -1.9111, -0.4942],\n",
      "         [ 1.1698, -2.4789,  0.7141]],\n",
      "\n",
      "        [[ 2.0960, -2.3606, -1.1278],\n",
      "         [ 1.2358, -2.2632, -0.9145],\n",
      "         [ 1.7210, -1.1523,  0.2450]],\n",
      "\n",
      "        [[ 1.8260, -3.1186, -0.2035],\n",
      "         [ 1.1445, -0.6755,  0.3767],\n",
      "         [ 1.1435, -1.0695, -1.0294]]])\n",
      "False\n",
      "tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Appendix - Check linear layer equivalence\n",
    "lin = nn.Linear(n_embd, n_embd)\n",
    "sd = lin.state_dict()\n",
    "for k, v in sd.items():\n",
    "    print(k, v.shape)\n",
    "sd['weight'] = c_proj_W.clone()\n",
    "sd['bias'] = c_proj_b.clone()\n",
    "lin.load_state_dict(sd)\n",
    "\n",
    "y = lin(x)\n",
    "print(y[:3,:3,:3])\n",
    "y2 = x @ c_proj_W.T + c_proj_b\n",
    "print(y2[:3,:3,:3])\n",
    "print(torch.allclose(y, y2))\n",
    "print((y - y2).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbadf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f3a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4d99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
