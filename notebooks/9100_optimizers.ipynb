{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf24e9aa",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Implementations for SDG, Momentum, etc. up to AdamW and Muon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c56216",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n",
    "\n",
    "**Idea:** Take a step in the negative gradient direction.\n",
    "\n",
    "```\n",
    "θ = θ - lr * g\n",
    "```\n",
    "\n",
    "That's it. Simple, but struggles with noisy gradients and ill-conditioned landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f553eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.data.add_(p.grad, alpha=-group['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ab2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr)\n",
    "opt_custom = SDG([W2], lr=lr)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a765b",
   "metadata": {},
   "source": [
    "## SGD with Momentum\n",
    "\n",
    "**Idea:** Accumulate gradients over time into a \"velocity.\" Smooths out noise, builds up speed in consistent directions.\n",
    "\n",
    "```\n",
    "v = β * v + g\n",
    "θ = θ - lr * v\n",
    "```\n",
    "\n",
    "Typical β = 0.9 (averages ~10 steps). Steady-state velocity is g/(1-β), so effective step is larger than vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDGMomentum(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        defaults = dict(lr=lr, momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'momentum_buffer': torch.zeros_like(p),\n",
    "                    }\n",
    "                # Update Step\n",
    "                buf = self.state[p]['momentum_buffer']\n",
    "                buf.mul_(group['momentum']).add_(p.grad)\n",
    "                p.data.add_(buf, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr, momentum=0.9)\n",
    "opt_custom = SDGMomentum([W2], lr=lr, momentum=0.9)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['momentum_buffer']\n",
    "    assert torch.equal(state1['momentum_buffer'], state2['momentum_buffer'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da797061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
