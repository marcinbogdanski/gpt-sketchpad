{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Character Level MLP - Activations, Gradients and BatchNorm</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's create a **MLP** model. Explore training and debugging techniques.\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33b0aa",
   "metadata": {},
   "source": [
    "# PyTorch-ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee29e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.weight[x]\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "        \n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        gain = 1.0 / (in_features**0.5)      # 1 / sqrt(fan_in)\n",
    "        self.weight = torch.randn((in_features, out_features)) * gain\n",
    "        self.bias = torch.randn((1, out_features)) * gain if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        self.out = out   # for debug/experiments\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1):\n",
    "        self.gain = torch.ones((1, num_features))\n",
    "        self.bias = torch.zeros((1, num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.mean_running = torch.zeros((1, num_features))\n",
    "        # TODO: shoudl track var for better eps behavior\n",
    "        self.std_running = torch.ones((1, num_features))\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            x_mean = torch.mean(x, dim=0, keepdim=True)\n",
    "            x_std = torch.std(x, dim=0, keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                self.mean_running = (1-self.momentum) * self.mean_running + self.momentum * x_mean\n",
    "                self.std_running = (1-self.momentum) * self.std_running + self.momentum * x_std\n",
    "        else:\n",
    "            x_mean = self.mean_running\n",
    "            x_std = self.std_running\n",
    "        zx = (x - x_mean) / (x_std + self.eps)\n",
    "        out = zx * self.gain + self.bias\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gain, self.bias]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "letters = ['.'] + letters\n",
    "n_vocab = len(letters)\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63733ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tok, block_size, names):\n",
    "    X, Y = [], []  # inputs and targets\n",
    "    for name in names:\n",
    "        name = '.'*block_size + name + '.'  # add start/stop tokens '..emma.'\n",
    "        for i in range(len(name) - block_size):\n",
    "            X.append(tok.encode(name[i:i+block_size]))\n",
    "            Y.append(tok.encode(name[i+block_size])[0])  # [0] to keep Y 1d tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length\n",
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tok, block_size, names[:n1])\n",
    "Xval, Yval = build_dataset(tok, block_size, names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(tok, block_size, names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335e632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2958)\n"
     ]
    }
   ],
   "source": [
    "# Expected initial loss:\n",
    "expected_initial_loss = -1 * torch.tensor(1/n_vocab).log()\n",
    "print(expected_initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "52318be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Init\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "n_embd = 10\n",
    "n_hid = 200\n",
    "\n",
    "embd = Embedding(n_vocab, n_embd)\n",
    "\n",
    "lin1 = Linear(n_embd*block_size, n_hid, bias=False)\n",
    "lin1.weight *= 5/3  # Kaiming init\n",
    "bn1 = BatchNorm1d(n_hid, momentum=0.001)\n",
    "tanh1 = Tanh()\n",
    "\n",
    "lin2 = Linear(n_hid, n_vocab)\n",
    "# TODO: change to standard pytorch\n",
    "lin2.weight *= (n_hid**0.5)\n",
    "lin2.weight *= 0.01\n",
    "lin2.bias *= 0.0     # disable bias on startup\n",
    "\n",
    "\n",
    "# params = [C, W1, b1, bngain1, bnbias1, W2, b2]\n",
    "# params = [C, W1, bngain1, bnbias1, W2, b2]\n",
    "params = [*embd.parameters(), *lin1.parameters(), *bn1.parameters(), *lin2.parameters()]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# No gradient calculation\n",
    "## bnmean1_running = torch.zeros((1, n_hid))\n",
    "## bnstd1_running = torch.ones((1, n_hid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "72fd8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, losses = [], []\n",
    "\n",
    "lr_schedule = [0.1]*100000 + [0.01]*100000\n",
    "num_epochs = len(lr_schedule)\n",
    "batch_size = 32\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc788989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.331486225128174\n",
      "10000 2.2780206203460693\n",
      "20000 2.2366952896118164\n",
      "30000 2.05989146232605\n",
      "40000 2.5080156326293945\n",
      "50000 1.9756540060043335\n",
      "60000 1.8901540040969849\n",
      "70000 2.3095462322235107\n",
      "80000 2.197767496109009\n",
      "90000 2.1742000579833984\n",
      "100000 2.2180747985839844\n",
      "110000 2.2235922813415527\n",
      "120000 1.857809066772461\n",
      "130000 2.1419620513916016\n",
      "140000 2.396003246307373\n",
      "150000 2.208874225616455\n",
      "160000 2.0063955783843994\n",
      "170000 1.9545999765396118\n",
      "180000 1.9888185262680054\n",
      "190000 1.7216124534606934\n",
      "200000 1.9555671215057373\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_epochs):\n",
    "\n",
    "    # Model Setup\n",
    "    bn1.training = True\n",
    "\n",
    "    # Random mini batch\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    x_batch = Xtr[batch_indices]\n",
    "    y_batch = Ytr[batch_indices]\n",
    "\n",
    "    # TODO: check all dims\n",
    "    # TODO: copy forward pass to eval and sampling\n",
    "\n",
    "    # Forward Pass\n",
    "    em = embd(x_batch)                         # n_batch, n_seq, n_emb\n",
    "    embcat = em.view(-1, n_embd*block_size)    # n_batch, n_embd*block_size\n",
    "    z1 = lin1(embcat)                          # n_batch, n_hid1\n",
    "    zz = bn1(z1)                               # n_batch, n_hid1\n",
    "    h1 = tanh1(zz)                             # n_batch, n_hid1\n",
    "    logits = lin2(h1)                          # n_batch, n_vocab\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward Pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = lr_schedule[i]\n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    iters.append(i)\n",
    "    losses.append(loss.item())\n",
    "    i += 1\n",
    "\n",
    "    # break\n",
    "\n",
    "print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee741b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_check():\n",
    "    with torch.no_grad():\n",
    "        my_std = sum(p.std() for p in params)\n",
    "        my_mean = sum(p.mean() for p in params)\n",
    "        my_sum = sum(p.sum() for p in params)\n",
    "        my_max = sum(p.max() for p in params)\n",
    "        my_min = sum(p.min() for p in params)\n",
    "\n",
    "    print(f\"{my_std=}\")\n",
    "    print(f\"{my_mean=}\")\n",
    "    print(f\"{my_sum=}\")\n",
    "    print(f\"{my_max=}\")\n",
    "    print(f\"{my_min=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5784f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_std=tensor(1.3207)\n",
      "my_mean=tensor(1.0633)\n",
      "my_sum=tensor(233.5514)\n",
      "my_max=tensor(4.3551)\n",
      "my_min=tensor(-2.7899)\n"
     ]
    }
   ],
   "source": [
    "print_train_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0597c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_std=tensor(1.3207)\n",
      "my_mean=tensor(1.0633)\n",
      "my_sum=tensor(233.5514)\n",
      "my_max=tensor(4.3551)\n",
      "my_min=tensor(-2.7899)\n"
     ]
    }
   ],
   "source": [
    "# Original:\n",
    "# my_std=tensor(1.3207)\n",
    "# my_mean=tensor(1.0633)\n",
    "# my_sum=tensor(233.5514)\n",
    "# my_max=tensor(4.3551)\n",
    "# my_min=tensor(-2.7899)\n",
    "print_train_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea79847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer activations\n",
    "plt.hist(z1.view(-1).tolist(), bins=100)\n",
    "plt.show()\n",
    "\n",
    "# hidden layer outputs\n",
    "plt.hist(h1.view(-1).tolist(), bins=100)\n",
    "plt.show()\n",
    "\n",
    "# neurons in tanh flat region\n",
    "plt.imshow(h1.abs() > 0.99, cmap='gray', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df403be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(iters, torch.log(torch.tensor(losses)))\n",
    "plt.plot(iters, torch.tensor(losses).log10())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6796289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0539)\n",
      "tensor(0.0266)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def calc_batch_norm_params_on_train_set():\n",
    "\n",
    "    # Model Setup\n",
    "    bn1.training = False\n",
    "\n",
    "    # Whole Dataset\n",
    "    x_batch = Xtr\n",
    "\n",
    "    # Forward Pass\n",
    "    em = embd(x_batch)                         # n_batch, n_seq, n_emb\n",
    "    embcat = em.view(-1, n_embd*block_size)    # n_batch, n_embd*block_size\n",
    "    z1 = lin1(embcat)                          # n_batch, n_hid1\n",
    "\n",
    "    # Batchnorm\n",
    "    z1_mean = torch.mean(z1, dim=0, keepdim=True)  # 1, n_hid1\n",
    "    z1_std = torch.std(z1, dim=0, keepdim=True)    # 1, n_hid1\n",
    "\n",
    "    return z1_std, z1_mean\n",
    "\n",
    "bnstd1, bnmean1 = calc_batch_norm_params_on_train_set()\n",
    "\n",
    "# Compare approaches\n",
    "print( torch.abs(bnstd1 - bn1.std_running).max() )\n",
    "print( torch.abs(bnmean1 - bn1.mean_running).max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc17a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train =  2.0673458576202393\n",
      "eval =   2.1093027591705322\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(x_batch, y_batch):\n",
    "    # Model Setup\n",
    "    bn1.training = False\n",
    "\n",
    "    # Forward Pass\n",
    "    em = embd(x_batch)                         # n_batch, n_seq, n_emb\n",
    "    embcat = em.view(-1, n_embd*block_size)    # n_batch, n_embd*block_size\n",
    "    z1 = lin1(embcat)                          # n_batch, n_hid1\n",
    "    zz = bn1(z1)                               # n_batch, n_hid1\n",
    "    h1 = tanh1(zz)                             # n_batch, n_hid1\n",
    "    logits = lin2(h1)                          # n_batch, n_vocab\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    return loss.item()\n",
    "\n",
    "print(\"train = \", evaluate(Xtr, Ytr))    # ~2.12\n",
    "print(\"eval =  \", evaluate(Xval, Yval))  # ~2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "98369278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base:\n",
    "# train =  2.1214349269866943\n",
    "# eval =   2.1567015647888184\n",
    "\n",
    "# Fixed W2*0.01 and b2*0.0 init:\n",
    "# train =  2.064913511276245\n",
    "# eval =   2.129284143447876\n",
    "\n",
    "# Fixed W1*0.2 and b1*0.01 init:\n",
    "# train =  2.0375447273254395\n",
    "# eval =   2.104278326034546\n",
    "\n",
    "# Kaming init\n",
    "# same as above, we went 0.2 -> 0.3 on W1, so not much difference\n",
    "# train =  2.0372910499572754\n",
    "# eval =   2.1173431873321533\n",
    "\n",
    "# Initial batchnorm (same, no gains expected, NN probably context limited)\n",
    "# train =  2.067298650741577\n",
    "# eval =   2.1195051670074463\n",
    "\n",
    "# Proper batchnorm (running mean/std, no linear layer bias)\n",
    "# train =  2.0653347969055176\n",
    "# eval =   2.121156930923462\n",
    "\n",
    "# Torch-ified batchnorm (classes)\n",
    "# train =  2.0673458576202393\n",
    "# eval =   2.1093027591705322"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eddf10e",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "68ff83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_name():\n",
    "    # Model Setup\n",
    "    bn1.training = False\n",
    "\n",
    "    context = tok.encode('.'*block_size)\n",
    "    while True:\n",
    "        # Construct Batch\n",
    "        x_batch = torch.tensor(context[-3:]).view(1, -1)   # n_batch=1, n_seq\n",
    "\n",
    "        # Forward Pass\n",
    "        em = embd(x_batch)                         # n_batch, n_seq, n_emb\n",
    "        embcat = em.view(-1, n_embd*block_size)    # n_batch, n_embd*block_size\n",
    "        z1 = lin1(embcat)                          # n_batch, n_hid1\n",
    "        zz = bn1(z1)                               # n_batch, n_hid1\n",
    "        h1 = tanh1(zz)                             # n_batch, n_hid1\n",
    "        logits = lin2(h1)                          # n_batch, n_vocab\n",
    "\n",
    "        # Probabilities        \n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample\n",
    "        sample = torch.multinomial(probs, 1).item()\n",
    "        context.append(sample)\n",
    "        \n",
    "        # Break\n",
    "        if sample == 0:  # stop token\n",
    "            break\n",
    "\n",
    "    return tok.decode(context)[block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccde1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anuelen.\n",
      "tia.\n",
      "marian.\n",
      "dan.\n",
      "shan.\n",
      "silaylen.\n",
      "kemah.\n",
      "lanie.\n",
      "epiacelle.\n",
      "jamiy.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "for i in range(10):\n",
    "    print(sample_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c8b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
