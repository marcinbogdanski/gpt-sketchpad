{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Building a WaveNet</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's create a **MLP** model. Explore training and debugging techniques.\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33b0aa",
   "metadata": {},
   "source": [
    "# PyTorch-ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee29e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "letters = ['.'] + letters\n",
    "n_vocab = len(letters)\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63733ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tok, n_seq, names):\n",
    "    X, Y = [], []  # inputs and targets\n",
    "    for name in names:\n",
    "        name = '.'*n_seq + name + '.'  # add start/stop tokens '..emma.'\n",
    "        for i in range(len(name) - n_seq):\n",
    "            X.append(tok.encode(name[i:i+n_seq]))\n",
    "            Y.append(tok.encode(name[i+n_seq])[0])  # [0] to keep Y 1d tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq = 8  # context length\n",
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tok, n_seq, names[:n1])\n",
    "Xval, Yval = build_dataset(tok, n_seq, names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(tok, n_seq, names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(Xtr[:20], Ytr[:20]):\n",
    "    print(tok.decode(x), tok.decode(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6589255",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf427f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = self.weight[x]\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.weight = torch.randn((in_features, out_features)) / in_features**0.5\n",
    "        self.bias = torch.zeros(out_features) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-05, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "\n",
    "        self.gain = torch.ones(dim)\n",
    "        self.bias = torch.zeros(dim)\n",
    "        \n",
    "        self.mean_running = torch.zeros(dim)\n",
    "        self.var_running = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0,1)\n",
    "            x_mean = torch.mean(x, dim=dim, keepdim=True)\n",
    "            x_var = torch.var(x, dim=dim, keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                self.mean_running = (1-self.momentum) * self.mean_running + self.momentum * x_mean\n",
    "                self.var_running = (1-self.momentum) * self.var_running + self.momentum * x_var\n",
    "        else:\n",
    "            x_mean = self.mean_running\n",
    "            x_var = self.var_running\n",
    "        zx = (x - x_mean) / (x_var + self.eps)**0.5  # sqrt\n",
    "        self.out = zx * self.gain + self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gain, self.bias]\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        n_batch, n_seq, n_emb = x.shape\n",
    "        x = x.view(n_batch, n_seq//self.n, n_emb*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)  # last conv n_seq==2, self.n==2, remove extra dim\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "\n",
    "    def train(self):\n",
    "        for l in self.layers: l.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        for l in self.layers: l.training = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected initial loss:\n",
    "expected_initial_loss = -1 * torch.tensor(1/n_vocab).log()\n",
    "print(expected_initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52318be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments:\n",
    "# - play with kaming init: 5/3\n",
    "# - play with weight init in Linear: / (in_features**0.5)\n",
    "\n",
    "# Random Init\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "# Model\n",
    "# n_embd = 10\n",
    "# n_hidden = 200\n",
    "# model = Sequential([  # 22097\n",
    "#     Embedding(n_vocab, n_embd),\n",
    "#     FlattenConsecutive(n=8),\n",
    "#     Linear(n_embd*n_seq, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "#     Linear(n_hidden, n_vocab),\n",
    "# ])\n",
    "\n",
    "\n",
    "# n_embd = 10\n",
    "# n_hidden = 68\n",
    "n_embd = 25\n",
    "n_hidden = 128\n",
    "model = Sequential([\n",
    "    Embedding(n_vocab, n_embd),\n",
    "    FlattenConsecutive(n=2), Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(n=2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(n=2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_vocab),\n",
    "])\n",
    "\n",
    "# Parameter Init\n",
    "with torch.no_grad():\n",
    "    model.layers[-1].weight *= 0.1     # last layer less confident for uniform softmax\n",
    "\n",
    "# Gather Params\n",
    "params = model.parameters()\n",
    "\n",
    "# Enable Grad\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Total Num Params\n",
    "print(sum(p.nelement() for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = torch.randint(0, Xtr.shape[0], (4,))\n",
    "x_batch, y_batch = Xtr[batch_indices], Ytr[batch_indices]\n",
    "logits = model(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    print(f\"{model.layers[i].__class__.__name__:>20}({str(i):<2}): {tuple(model.layers[i].out.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, losses, ud = [], [], []\n",
    "\n",
    "lr_schedule = [0.1]*150_000 + [0.01]*50_000\n",
    "num_epochs = len(lr_schedule)\n",
    "n_batch = 32\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc788989",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "for _ in range(num_epochs):\n",
    "\n",
    "    # Set Mode\n",
    "    model.train()\n",
    "\n",
    "    # Mini Batch\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (n_batch,))\n",
    "    x_batch, y_batch = Xtr[batch_indices], Ytr[batch_indices]\n",
    "\n",
    "    # Forward\n",
    "    logits = model(x_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = lr_schedule[i]\n",
    "    for p in params:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # Stats\n",
    "    if i % 10000 == 0:\n",
    "        time_taken = time.time() - time_start\n",
    "        time_start = time.time()\n",
    "        print(f\"{time_taken:.2f}  {i}  {loss.item()}\")\n",
    "    iters.append(i)\n",
    "    losses.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr*p.grad.std()/p.data.std()).log10().item() for p in params])\n",
    "\n",
    "    # Break\n",
    "    if i % 10_000 == 0:\n",
    "        break\n",
    "    break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88067bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(losses).view(-1, 1000).mean(dim=-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(x_batch, y_batch):\n",
    "    model.eval()\n",
    "    logits = model(x_batch)\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "    return loss.item()\n",
    "\n",
    "print(\"train = \", evaluate(Xtr, Ytr))    # ~2.12\n",
    "print(\"eval =  \", evaluate(Xval, Yval))  # ~2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_name():\n",
    "    model.eval()    \n",
    "    context = tok.encode('.'*block_size)\n",
    "    while True:\n",
    "        x = torch.tensor(context[-3:]).view(1, -1)   # n_batch=1, n_seq\n",
    "        # Forward Pass\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        # Sample\n",
    "        sample = torch.multinomial(probs, 1).item()\n",
    "        context.append(sample)\n",
    "        # Break\n",
    "        if sample == 0:  # stop token\n",
    "            break\n",
    "\n",
    "    return tok.decode(context)[block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "for i in range(10):\n",
    "    print(sample_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d12a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
