{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e92073",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Character Level Bigram Model - Gradient Descent</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01ca69",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's create a **bigram** model by **gradient descent** - a single linear layer pseudo neural network.\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07433482",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cce51",
   "metadata": {},
   "source": [
    "Let's define:\n",
    "\n",
    "Vector of scores, i.e. logits, i.e. row in matrix $W$:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = (z_1, \\dots, z_K)\n",
    "$$\n",
    "\n",
    "Model predictions, using softmax:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat y} = (\\hat y_1, \\dots, \\hat y_K),\n",
    "\\qquad\n",
    "\\hat y_i = \\mathcal{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}{e^{z_j}}}\n",
    "$$\n",
    "\n",
    "One-hot target labels\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = (y_1, \\dots, y_K ),\n",
    "\\qquad\n",
    "y_k =\n",
    "\\begin{cases}\n",
    "1 & \\text{for the correct class } k, \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Cross-entropy loss, general and simplified form for correct class $\\mathcal{c}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{k=1}^{K} y_k \\log{\\hat y_k},\n",
    "\\qquad\n",
    "\\mathcal{L} = -\\log{\\hat y_c}\n",
    "$$\n",
    "\n",
    "Gradient w.r.t logits:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447baba4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c6c2d",
   "metadata": {},
   "source": [
    "# Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f04c0f",
   "metadata": {},
   "source": [
    "Load the data and show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce60b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3022f",
   "metadata": {},
   "source": [
    "Count the bigram pairs, including special start/stop tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88139507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Confirm the vocabulary is ASCII only\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "\n",
    "# Add start/stop tokens - same token for both\n",
    "letters = ['.'] + letters\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5a0a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 0), ('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5), ('f', 6), ('g', 7), ('h', 8), ('i', 9)]\n",
      "[(0, '.'), (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h'), (9, 'i')]\n"
     ]
    }
   ],
   "source": [
    "# Indices for all characters, including start/stop tokens\n",
    "stoi = {ch: i for i, ch in enumerate(letters)}\n",
    "itos = {i: ch for i, ch in enumerate(letters)}\n",
    "# Print first 10 entries to verify\n",
    "print(list(stoi.items())[:10])\n",
    "print(list(itos.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b3b3a",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3ac5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 228146\n",
      "X (228146,),int64:\n",
      "[ 0  5 13 13  1  0 15 12  9 22]\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []  # inputs and targets\n",
    "\n",
    "for name in names:\n",
    "    name = '.' + name + '.'  # add start/stop tokens\n",
    "    for i in range(len(name) - 1):\n",
    "        first_char = name[i]\n",
    "        second_char = name[i + 1]\n",
    "        X.append(first_char)\n",
    "        Y.append(second_char)\n",
    "\n",
    "X = np.array([stoi[c] for c in X])\n",
    "Y = np.array([stoi[c] for c in Y])\n",
    "\n",
    "print(\"Num examples:\", len(X))\n",
    "print(f\"X {X.shape},{X.dtype}:\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52b97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5 13 13  1  0 15 12  9 22]\n",
      "['.', 'e', 'm', 'm', 'a', '.', 'o', 'l', 'i', 'v']\n",
      "[ 5 13 13  1  0 15 12  9 22  9]\n",
      "['e', 'm', 'm', 'a', '.', 'o', 'l', 'i', 'v', 'i']\n"
     ]
    }
   ],
   "source": [
    "print(X[:10])\n",
    "print([itos[i] for i in X[:10]])\n",
    "print(Y[:10])\n",
    "print([itos[i] for i in Y[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558c907",
   "metadata": {},
   "source": [
    "# Optimize - Single Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c642c",
   "metadata": {},
   "source": [
    "Reduce data size for easier printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283e45b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (6,) int64:\n",
      "[ 0  5 13 13  1  0]\n",
      "\n",
      "y (6,) int64:\n",
      "[ 5 13 13  1  0 15]\n",
      "\n",
      "x_one_hot (6, 27) float32\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "y_one_hot (6, 27) float32\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = X[:6]\n",
    "y = Y[:6]\n",
    "\n",
    "print(f\"x {x.shape} {x.dtype}:\")\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(f\"y {y.shape} {y.dtype}:\")\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "x_one_hot = np.zeros((len(x), len(letters)), dtype=np.float32)\n",
    "x_one_hot[np.arange(len(x)), x] = 1\n",
    "print(f\"x_one_hot {x_one_hot.shape} {x_one_hot.dtype}\")\n",
    "print(x_one_hot[:5])\n",
    "print()\n",
    "\n",
    "y_one_hot = np.zeros((len(y), len(letters)), dtype=np.float32)\n",
    "y_one_hot[np.arange(len(y)), y] = 1\n",
    "print(f\"y_one_hot {y_one_hot.shape} {y_one_hot.dtype}\")\n",
    "print(y_one_hot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eab4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (27, 27),float32:\n",
      "[[-0.09 -1.46  1.08 -0.24 -0.49 -1.    0.92 -1.1   0.63 -0.56  0.03 -0.23  0.59  0.75 -1.06  1.06  0.75  1.06  1.52 -1.49  1.86 -1.6  -0.65  0.34  1.05  0.63  0.36]\n",
      " [ 0.56 -1.09  0.02  2.5  -2.49 -0.23 -0.1  -0.89 -0.14  0.1  -0.25 -0.08 -1.09  0.59 -0.64 -1.11  2.11 -0.57 -0.48 -1.92  0.4  -1.05 -0.69  0.75  0.54 -0.73  0.56]\n",
      " [ 0.43 -0.14 -0.94  0.48 -1.53  0.4   0.01 -1.23 -1.05  2.52 -2.04  0.09 -0.31  0.49  0.35  0.95  0.76  0.01 -1.38 -0.27  0.54  0.54  1.16 -0.17 -1.18 -0.55  0.27]\n",
      " [ 0.98  1.01  0.78 -1.25 -0.42  0.55  0.33  0.86 -1.23  0.62 -2.77 -0.49  0.07 -0.35  0.87 -0.22  0.02  0.69 -0.88  1.5  -0.65  0.6   0.21 -0.42  0.1   0.31 -0.47]\n",
      " [ 2.2  -1.01  0.    1.13  0.51  1.08 -1.53 -0.23  0.04  1.26 -0.82 -0.06  1.88  0.38  0.43 -0.06  1.19 -1.87 -0.82 -0.57  0.13  0.   -2.13  0.11 -0.85  2.83 -0.27]\n",
      " [ 1.06 -0.55 -0.24 -0.85  0.85  1.33 -1.27  0.52  0.78  1.39 -0.47  1.59 -1.09  0.36 -0.33 -0.13  1.26  0.7   3.08  0.09 -0.58  1.35  0.42 -0.28  1.49  1.63 -1.31]\n",
      " [-0.21  1.36 -0.59  1.88  0.04  0.6   0.35 -0.93  0.01 -0.53 -0.25  1.26  0.84 -0.06  0.45  1.7   0.14 -0.24  0.91 -0.4  -1.07  2.05 -0.96  0.32  0.6  -0.16  2.07]\n",
      " [ 2.11  0.59 -1.39  0.56 -1.6   0.73  1.02  0.48  1.44  0.35 -0.78 -0.31  1.25 -2.11  0.34 -1.24  2.38 -1.53 -2.35  0.92  0.85 -0.83 -0.28 -1.52 -0.34  0.71  0.16]\n",
      " [-0.2  -0.75  1.04  1.78  1.44 -1.37  0.2  -0.83 -0.32  0.48  2.85  0.4   0.09 -0.44  0.7  -1.6   0.83  1.01 -0.87  1.76  2.47 -0.7   0.93 -0.39 -0.61  0.97  0.46]\n",
      " [-1.23 -1.47  0.32 -1.38 -1.23 -0.17 -0.41  0.5   2.17 -1.21  1.64 -1.33  0.15  1.21  2.59 -1.64  1.87  0.01  0.28  0.68  2.08  2.65  0.55 -0.51 -0.7  -2.19 -1.54]\n",
      " [ 1.2  -1.71 -1.22 -0.54 -0.68 -0.87 -1.01 -0.58  1.56  0.8   0.2  -0.67  0.2   0.36 -0.83 -1.13  0.32 -0.71 -0.75  0.01 -0.04 -1.    1.7   0.35 -0.16  0.79  1.47]\n",
      " [ 0.05 -2.64 -1.25 -0.05  0.16  1.49  0.56 -0.26  0.89 -0.62  1.05 -1.58  1.65 -0.13  0.92 -0.21 -1.41 -0.19  0.08  0.28  0.16  0.46 -0.55 -1.72 -0.43 -1.4  -0.17]\n",
      " [-0.27  0.09 -0.52  0.67  0.63  0.74  0.43  0.82  1.01  0.14 -2.22 -0.6   1.7  -2.18  0.17 -0.55  1.58  0.47 -0.21  0.08 -1.65  2.12  0.82 -0.85 -0.51  0.66 -0.62]\n",
      " [-0.78  0.96 -0.04  0.51  1.05 -0.74  1.7   1.07 -0.96 -1.22  0.23  0.44 -0.63 -0.15 -0.12 -0.25  0.5   1.69  2.05 -0.69  0.08 -0.48  0.06 -0.28  1.52  1.1  -0.46]\n",
      " [ 1.51 -0.57  0.27  0.73 -0.23 -0.63  0.47  0.96 -0.01  1.95  0.86 -0.41  0.88 -0.17  0.61  0.82  0.67  1.75  0.35  0.66 -0.15 -1.38 -0.44  0.25 -2.49  0.44  0.51]\n",
      " [ 1.24  0.5   1.23  0.1  -0.35  0.64 -0.96  1.44 -1.1   1.55 -0.51 -0.42 -1.02 -0.16  0.16 -1.46 -1.74  0.01 -1.47  0.66  1.21 -0.52  3.27 -0.01 -1.07  1.91  0.49]\n",
      " [-0.51 -0.04  1.43  1.65 -0.01 -1.07 -0.54  0.72  0.06  0.36  0.38  1.62 -0.32 -0.12 -2.13 -0.15 -1.78 -0.77  0.08 -0.62 -0.12 -2.28 -0.42  0.17  0.01  0.42 -0.72]\n",
      " [ 0.69 -0.57 -0.27  1.01  1.46  0.87 -1.12 -0.32 -0.48  0.11  1.64  1.19 -1.2   0.37  0.28 -0.65 -0.37  1.21 -0.13 -0.13 -0.43 -3.39  0.15  0.96  2.11  2.44 -0.95]\n",
      " [ 0.04 -0.49  0.35 -1.02  0.34  0.72  0.69  0.46  1.09 -0.17 -0.38 -0.01 -0.25  0.43  1.12 -0.07 -1.69 -1.64  0.47  0.65  0.58 -2.62  0.83  2.82  0.02 -0.18  1.18]\n",
      " [-0.69 -1.02  1.25 -0.91  0.73 -0.5  -0.27 -0.87  0.55 -0.02 -0.12 -0.41 -0.54  1.15  0.87  1.02 -1.13  0.47 -1.66  1.62  1.85  1.45  0.53  0.61 -0.13 -0.79  0.92]\n",
      " [ 1.08 -0.41 -1.4  -0.55 -0.42 -0.71 -1.9  -1.53 -0.01  1.85 -1.52  0.11 -0.38  0.28  0.02  1.08  0.   -0.74  0.16  0.06  0.81  0.54  0.55 -0.02 -0.22 -0.32  0.16]\n",
      " [ 1.19  1.48 -0.65 -0.23 -0.05  1.83 -0.89  0.07 -2.39  0.35 -0.96 -0.02  0.92 -1.4  -2.37  0.4  -0.66  0.51  1.5  -1.02 -1.35 -0.91 -1.79 -0.85  1.07 -0.87 -1.56]\n",
      " [ 0.38  0.51 -2.17  0.06 -0.54  0.38 -0.83 -0.48 -1.17  2.24  0.57  0.77  0.76  0.16  1.38  0.61  2.72  0.97  1.71 -1.29 -0.49 -0.39 -1.44 -0.48 -0.44 -1.1   0.87]\n",
      " [-0.54  0.17 -1.5  -0.18 -0.74 -0.73  1.1   0.45 -1.01 -0.91  0.84 -0.74 -0.16  1.46  1.32  0.09  0.02  0.3  -0.14  0.09  1.31  0.47  0.32 -0.11  1.02 -0.4   0.56]\n",
      " [-1.04 -0.27  0.37 -0.56  0.31  0.14 -0.43  0.7   0.3  -0.39  0.2   0.91 -0.98 -1.22  0.91 -0.99 -0.6  -2.28 -0.33  0.16 -1.52  0.28 -1.38 -0.51  1.08  1.5  -0.23]\n",
      " [ 1.2  -0.74  0.85 -1.68  0.38 -0.35 -1.11 -0.3  -0.43 -0.24  0.58 -0.87 -0.84 -0.13 -0.64 -0.16 -1.34  0.69 -1.09  0.8   0.74 -0.82  0.57  1.28 -0.33  0.34 -0.99]\n",
      " [ 1.    0.14  1.51  0.89 -0.06 -0.93  0.84  1.09 -0.35  0.92  0.4  -0.31 -0.78 -0.12  0.93 -0.98 -0.16 -0.16 -0.76 -1.01  0.36  1.   -0.31 -1.21 -0.05 -0.41  0.16]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(22)\n",
    "\n",
    "# Init Weights\n",
    "W = np.random.randn(len(letters), len(letters)).astype(np.float32)\n",
    "print(f\"W {W.shape},{W.dtype}:\")\n",
    "print(W.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fda371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits (6, 27),float32:\n",
      "[[-0.09 -1.46  1.08 -0.24 -0.49 -1.    0.92 -1.1   0.63 -0.56  0.03 -0.23  0.59  0.75 -1.06  1.06  0.75  1.06  1.52 -1.49  1.86 -1.6  -0.65  0.34  1.05  0.63  0.36]\n",
      " [ 1.06 -0.55 -0.24 -0.85  0.85  1.33 -1.27  0.52  0.78  1.39 -0.47  1.59 -1.09  0.36 -0.33 -0.13  1.26  0.7   3.08  0.09 -0.58  1.35  0.42 -0.28  1.49  1.63 -1.31]\n",
      " [-0.78  0.96 -0.04  0.51  1.05 -0.74  1.7   1.07 -0.96 -1.22  0.23  0.44 -0.63 -0.15 -0.12 -0.25  0.5   1.69  2.05 -0.69  0.08 -0.48  0.06 -0.28  1.52  1.1  -0.46]\n",
      " [-0.78  0.96 -0.04  0.51  1.05 -0.74  1.7   1.07 -0.96 -1.22  0.23  0.44 -0.63 -0.15 -0.12 -0.25  0.5   1.69  2.05 -0.69  0.08 -0.48  0.06 -0.28  1.52  1.1  -0.46]\n",
      " [ 0.56 -1.09  0.02  2.5  -2.49 -0.23 -0.1  -0.89 -0.14  0.1  -0.25 -0.08 -1.09  0.59 -0.64 -1.11  2.11 -0.57 -0.48 -1.92  0.4  -1.05 -0.69  0.75  0.54 -0.73  0.56]\n",
      " [-0.09 -1.46  1.08 -0.24 -0.49 -1.    0.92 -1.1   0.63 -0.56  0.03 -0.23  0.59  0.75 -1.06  1.06  0.75  1.06  1.52 -1.49  1.86 -1.6  -0.65  0.34  1.05  0.63  0.36]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Logits\n",
    "logits = x_one_hot @ W  # n_batch, n_vocab\n",
    "\n",
    "# Equivalently\n",
    "logits_2 = W[x,:]    # n_batch, n_vocab\n",
    "assert np.allclose(logits, logits_2)\n",
    "del logits_2\n",
    "\n",
    "print(f\"logits {logits.shape},{logits.dtype}:\")\n",
    "print(logits.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ac7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    max_ = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp(logits - max_)\n",
    "    exp_sum = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp / exp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de43393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat (6, 27),float32:\n",
      "[[0.02 0.01 0.07 0.02 0.01 0.01 0.06 0.01 0.04 0.01 0.02 0.02 0.04 0.05 0.01 0.06 0.05 0.06 0.1  0.01 0.14 0.   0.01 0.03 0.06 0.04 0.03]\n",
      " [0.04 0.01 0.01 0.01 0.03 0.05 0.   0.02 0.03 0.06 0.01 0.07 0.   0.02 0.01 0.01 0.05 0.03 0.3  0.02 0.01 0.05 0.02 0.01 0.06 0.07 0.  ]\n",
      " [0.01 0.05 0.02 0.03 0.06 0.01 0.11 0.06 0.01 0.01 0.02 0.03 0.01 0.02 0.02 0.02 0.03 0.11 0.15 0.01 0.02 0.01 0.02 0.01 0.09 0.06 0.01]\n",
      " [0.01 0.05 0.02 0.03 0.06 0.01 0.11 0.06 0.01 0.01 0.02 0.03 0.01 0.02 0.02 0.02 0.03 0.11 0.15 0.01 0.02 0.01 0.02 0.01 0.09 0.06 0.01]\n",
      " [0.04 0.01 0.02 0.29 0.   0.02 0.02 0.01 0.02 0.03 0.02 0.02 0.01 0.04 0.01 0.01 0.2  0.01 0.01 0.   0.04 0.01 0.01 0.05 0.04 0.01 0.04]\n",
      " [0.02 0.01 0.07 0.02 0.01 0.01 0.06 0.01 0.04 0.01 0.02 0.02 0.04 0.05 0.01 0.06 0.05 0.06 0.1  0.01 0.14 0.   0.01 0.03 0.06 0.04 0.03]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = softmax(logits)\n",
    "print(f\"y_hat {y_hat.shape},{y_hat.dtype}:\")\n",
    "print(y_hat.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e65b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(y_hat.sum(axis=-1, keepdims=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bbba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, correct_target_idx):\n",
    "    \"\"\"Compute the cross-entropy loss. Equivalent to neg log likelihood.\"\"\"\n",
    "    target_class_prob = y_hat[np.arange(len(y_hat)), correct_target_idx]    # n_batch\n",
    "    ce_loss = -1 * np.log(target_class_prob)\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbbb5974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.618154525756836\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.01\n",
    "loss = cross_entropy(y_hat, y).mean()\n",
    "loss = loss.item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97668b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_name(BP):\n",
    "    \"\"\"Sample a name using the probabilities table.\"\"\"\n",
    "    chars = [\".\"]   # start token\n",
    "    while True:\n",
    "        current_char = chars[-1]\n",
    "        current_int = stoi[current_char]\n",
    "        row_probs = BP[current_int]\n",
    "        next_int = np.random.choice(len(row_probs), p=row_probs)\n",
    "        next_char = itos[next_int]\n",
    "        chars.append(next_char)\n",
    "        if next_char == \".\":   # end token\n",
    "            break\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a676cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".hjluacerht.\n",
      ".qyua.\n",
      ".tytiupgpgnqdyznq.\n",
      ".fzqqdlpc.\n",
      ".obaptcajhtotznfcniu.\n",
      ".lgpstht.\n",
      ".myxapbmfcnnjpdyewrwapziuehtu.\n",
      ".tluehtovaposhdytvitlovawmq.\n",
      ".bwgpgpkulacsovrsuotiuicbvaj.\n",
      ".qxbwmyjslerzineanjyjhzt.\n",
      ".lcsjisuqkqkndywnf.\n",
      ".qqbmywqxxkjvehcztlf.\n",
      ".tlyqdrbksotwtobrmxrrwflxnlgpcyerwttobpybuermqninqxjckfqy.\n",
      ".bcig.\n",
      ".rwmrerzuudzg.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(22)\n",
    "\n",
    "# Sample from model\n",
    "P = softmax(W)\n",
    "for i in range(15):\n",
    "    name = sample_name(BP=P)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e00005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_grad (6, 27),float32:\n",
      "[[ 0.02  0.01  0.07  0.02  0.01 -0.99  0.06  0.01  0.04  0.01  0.02  0.02  0.04  0.05  0.01  0.06  0.05  0.06  0.1   0.01  0.14  0.    0.01  0.03  0.06  0.04  0.03]\n",
      " [ 0.04  0.01  0.01  0.01  0.03  0.05  0.    0.02  0.03  0.06  0.01  0.07  0.   -0.98  0.01  0.01  0.05  0.03  0.3   0.02  0.01  0.05  0.02  0.01  0.06  0.07  0.  ]\n",
      " [ 0.01  0.05  0.02  0.03  0.06  0.01  0.11  0.06  0.01  0.01  0.02  0.03  0.01 -0.98  0.02  0.02  0.03  0.11  0.15  0.01  0.02  0.01  0.02  0.01  0.09  0.06  0.01]\n",
      " [ 0.01 -0.95  0.02  0.03  0.06  0.01  0.11  0.06  0.01  0.01  0.02  0.03  0.01  0.02  0.02  0.02  0.03  0.11  0.15  0.01  0.02  0.01  0.02  0.01  0.09  0.06  0.01]\n",
      " [-0.96  0.01  0.02  0.29  0.    0.02  0.02  0.01  0.02  0.03  0.02  0.02  0.01  0.04  0.01  0.01  0.2   0.01  0.01  0.    0.04  0.01  0.01  0.05  0.04  0.01  0.04]\n",
      " [ 0.02  0.01  0.07  0.02  0.01  0.01  0.06  0.01  0.04  0.01  0.02  0.02  0.04  0.05  0.01 -0.94  0.05  0.06  0.1   0.01  0.14  0.    0.01  0.03  0.06  0.04  0.03]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backward pass into logits\n",
    "logits_grad = y_hat - y_one_hot\n",
    "\n",
    "# Equivalently\n",
    "logits_grad_2 = y_hat.copy()\n",
    "logits_grad_2[np.arange(len(y)), y] -= 1\n",
    "assert np.allclose(logits_grad, logits_grad_2)\n",
    "del logits_grad_2\n",
    "\n",
    "print(f\"logits_grad {logits_grad.shape},{logits_grad.dtype}:\")\n",
    "print(logits_grad.round(2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29a87c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad (27, 27),float32:\n",
      "[[ 0.01  0.    0.02  0.01  0.   -0.16  0.02  0.    0.01  0.    0.01  0.01  0.01  0.02  0.   -0.15  0.02  0.02  0.03  0.    0.05  0.    0.    0.01  0.02  0.01  0.01]\n",
      " [-0.16  0.    0.    0.05  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.    0.    0.03  0.    0.    0.    0.01  0.    0.    0.01  0.01  0.    0.01]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.01  0.    0.    0.    0.01  0.01  0.    0.    0.01  0.01  0.    0.01  0.   -0.16  0.    0.    0.01  0.    0.05  0.    0.    0.01  0.    0.    0.01  0.01  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.   -0.15  0.01  0.01  0.02  0.    0.04  0.02  0.    0.    0.01  0.01  0.   -0.16  0.01  0.01  0.01  0.04  0.05  0.    0.01  0.    0.01  0.    0.03  0.02  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Backward pass into W\n",
    "W_grad = x_one_hot.T @ logits_grad / len(x)\n",
    "\n",
    "# Equivalently, accumulate by indexing\n",
    "# Note: one-hot version on this tiny dataset is actually faster because\n",
    "# matmuls use highly optmised BLAS routines, while np.add.at is sequential\n",
    "W_grad_2 = np.zeros_like(W)\n",
    "np.add.at(W_grad_2, x, logits_grad)\n",
    "W_grad_2 /= len(x)\n",
    "assert np.allclose(W_grad, W_grad_2)\n",
    "del W_grad_2\n",
    "\n",
    "print(f\"W_grad {W_grad.shape},{W_grad.dtype}:\")\n",
    "print(W_grad.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df8021c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float128\n",
      "Numerical grad check ok!\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient_check():\n",
    "\n",
    "    np.random.seed(22)\n",
    "\n",
    "    # Init Weights\n",
    "    W = np.random.randn(len(letters), len(letters)).astype(np.float128)\n",
    "    print(W.dtype)\n",
    "\n",
    "    eps = 1e-5\n",
    "\n",
    "    # Forward pass\n",
    "    logits = W[x,:]    # n_batch, n_vocab\n",
    "    y_hat = softmax(logits)\n",
    "    loss = cross_entropy(y_hat, y).mean() + weight_decay*(W**2).mean()\n",
    "    loss = loss.item()\n",
    "\n",
    "    # Backward pass\n",
    "    # Equivalent to: logits_grad = y_hat - y_one_hot\n",
    "    logits_grad = y_hat.copy()\n",
    "    logits_grad[np.arange(len(y)), y] -= 1\n",
    "\n",
    "    # Equivalent to: W_grad = x_one_hot.T @ logits_grad / len(x)\n",
    "    W_grad = np.zeros_like(W)\n",
    "    np.add.at(W_grad, x, logits_grad)\n",
    "    W_grad /= len(x)\n",
    "    W_grad += 2*weight_decay*W / W.size  # Gradient from weight_decay\n",
    "\n",
    "    # Grad check\n",
    "    W_grad_num = np.zeros_like(W)\n",
    "    i, j = 0, 0\n",
    "\n",
    "    # for i in range(W.shape[0]):\n",
    "    #     for j in range(W.shape[1]):\n",
    "\n",
    "    i, j = 0, 0\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_cpy_minus = W.copy()\n",
    "            W_cpy_minus[i, j] -= eps\n",
    "            logits = W_cpy_minus[x,:]    # n_batch, n_vocab\n",
    "            y_hat = softmax(logits)\n",
    "            loss_minus = cross_entropy(y_hat, y).mean()\n",
    "\n",
    "            W_cpy_plus = W.copy()\n",
    "            W_cpy_plus[i, j] += eps\n",
    "            logits = W_cpy_plus[x,:]    # n_batch, n_vocab\n",
    "            y_hat = softmax(logits)\n",
    "            loss_plus = cross_entropy(y_hat, y).mean()\n",
    "\n",
    "            W_grad_num[i, j] = (loss_plus - loss_minus) / (2*eps)\n",
    "\n",
    "    #assert np.allclose(W_grad, W_grad_num)\n",
    "    return np.allclose(W_grad, W_grad_num, atol=1e-4, rtol=1e-3)\n",
    "\n",
    "if numerical_gradient_check():\n",
    "    print(\"Numerical grad check ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30073f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before the update: 3.618154525756836\n",
      "loss after one update: 3.6011502742767334\n"
     ]
    }
   ],
   "source": [
    "print(\"loss before the update:\", loss)\n",
    "\n",
    "# Update weights\n",
    "learning_rate = 0.1\n",
    "W += -learning_rate * W_grad\n",
    "\n",
    "loss = cross_entropy(softmax(W[x,:]), y).mean().item()\n",
    "print(\"loss after one update:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575319f4",
   "metadata": {},
   "source": [
    "# Train Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92bb751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, learning_rate=10, weight_init_scale=1.0, weight_decay=0.01, early_stop=2.49, print_every=100, max_epochs=500):\n",
    "\n",
    "    print(f\"Training model with: \"\n",
    "          f\"lr={learning_rate}, \"\n",
    "          f\"weight_init_scale={weight_init_scale}, \"\n",
    "          f\"max_epochs={max_epochs}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Copy not technically necessery\n",
    "    x = X.copy()\n",
    "    y = Y.copy()\n",
    "\n",
    "    np.random.seed(22)\n",
    "    W = np.random.randn(len(letters), len(letters)).astype(np.float32) * weight_init_scale\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Forward pass\n",
    "        logits = W[x,:]    # n_batch, n_vocab\n",
    "        y_hat = softmax(logits)\n",
    "        loss = cross_entropy(y_hat, y).mean() + weight_decay*(W**2).mean()\n",
    "        loss = loss.item()\n",
    "\n",
    "        # Print and store loss\n",
    "        if print_every is not None and epoch % print_every == 0:\n",
    "            print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backward pass\n",
    "        # Equivalent to: logits_grad = y_hat - y_one_hot\n",
    "        logits_grad = y_hat.copy()\n",
    "        logits_grad[np.arange(len(y)), y] -= 1\n",
    "\n",
    "        # Equivalent to: W_grad = x_one_hot.T @ logits_grad / len(x)\n",
    "        W_grad = np.zeros_like(W)\n",
    "        np.add.at(W_grad, x, logits_grad)\n",
    "        W_grad /= len(x)\n",
    "        W_grad += 2*weight_decay*W / W.size  # Gradient from weight_decay\n",
    "\n",
    "        # Update weights\n",
    "        W += -learning_rate * W_grad\n",
    "\n",
    "        if loss < early_stop:\n",
    "            break\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Training completed: \"\n",
    "          f\"elapsed_time={elapsed_time:.2f}s, \"\n",
    "          f\"final_epoch={epoch}, loss={loss}\")\n",
    "    \n",
    "    result = {\n",
    "        'name': f'w={weight_init_scale}_lr={learning_rate}',\n",
    "        'W': W,\n",
    "        'losses': losses,\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90781802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with: lr=25.0, weight_init_scale=0.01, max_epochs=500\n",
      "epoch: 0, loss: 3.2961506843566895\n",
      "epoch: 50, loss: 2.5306262969970703\n",
      "epoch: 100, loss: 2.500636339187622\n",
      "epoch: 150, loss: 2.4911513328552246\n",
      "Training completed: elapsed_time=31.99s, final_epoch=161, loss=2.4899325370788574\n"
     ]
    }
   ],
   "source": [
    "result = train_model(X, Y, learning_rate=25.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, print_every=50, max_epochs=500)\n",
    "W = result['W']\n",
    "losses = result['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b703c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".chasah.\n",
      ".mar.\n",
      ".kora.\n",
      ".ryn.\n",
      ".quliemi.\n"
     ]
    }
   ],
   "source": [
    "# Init numpy random seed\n",
    "np.random.seed(22)\n",
    "\n",
    "P = softmax(W)\n",
    "for i in range(5):\n",
    "    name = sample_name(BP=P)\n",
    "    print(name)\n",
    "\n",
    "# Expected output:\n",
    "# .chasah.\n",
    "# .mar.\n",
    "# .kora.\n",
    "# .ryn.\n",
    "# .quliemi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a25b91",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_runs = {}\n",
    "\n",
    "result = train_model(X, Y, learning_rate=150.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=25.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=10.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=1.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, losses in train_runs.items():\n",
    "    ax.plot(losses, label=name)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_runs = {}\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=2.0, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=1.0, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=0.1, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=0.01, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']\n",
    "\n",
    "result = train_model(X, Y, learning_rate=50.0, weight_init_scale=0.0, weight_decay=0.01, early_stop=2.49, max_epochs=500, print_every=None)\n",
    "train_runs[result['name']] = result['losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, losses in train_runs.items():\n",
    "    ax.plot(losses, label=name)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ee48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86761654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
