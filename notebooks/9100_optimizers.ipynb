{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf24e9aa",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Implementations for SDG, Momentum, etc. up to AdamW and Muon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c56216",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n",
    "\n",
    "**Idea:** Take a step in the negative gradient direction.\n",
    "\n",
    "```\n",
    "θ = θ - lr * g\n",
    "```\n",
    "\n",
    "That's it. Simple, but struggles with noisy gradients and ill-conditioned landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f553eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "                # Update\n",
    "                p.data.add_(grad, alpha=-group['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ab2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr, weight_decay=0.01)\n",
    "opt_custom = SDG([W2], lr=lr, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a765b",
   "metadata": {},
   "source": [
    "## SGD with Momentum\n",
    "\n",
    "**Idea:** Accumulate gradients over time into a \"velocity.\" Smooths out noise, builds up speed in consistent directions.\n",
    "\n",
    "```\n",
    "v = β * v + g\n",
    "θ = θ - lr * v\n",
    "```\n",
    "\n",
    "Typical β = 0.9 (averages ~10 steps). Steady-state velocity is g/(1-β), so effective step is larger than vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDGMomentum(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'momentum_buffer': torch.zeros_like(p),\n",
    "                    }\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "                # Update Step\n",
    "                v = self.state[p]['momentum_buffer']\n",
    "                v.mul_(group['momentum']).add_(grad)\n",
    "                p.data.add_(v, alpha=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.SGD([W1], lr=lr, momentum=0.9, weight_decay=0.01)\n",
    "opt_custom = SDGMomentum([W2], lr=lr, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['momentum_buffer']\n",
    "    assert torch.equal(state1['momentum_buffer'], state2['momentum_buffer'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15721b0b",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "**Idea:** Fix AdaGrad by using EMA instead of sum. Old gradients decay away.\n",
    "\n",
    "```\n",
    "s = β * s + (1 - β) * g²\n",
    "θ = θ - lr * g / (√s + ε)\n",
    "```\n",
    "\n",
    "Typical β = 0.99. Learning rate stabilizes instead of decaying to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'square_avg': torch.zeros_like(p),\n",
    "                    }\n",
    "\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "\n",
    "                # Update Step\n",
    "                s = self.state[p]['square_avg']\n",
    "                s.mul_(group['alpha'])\n",
    "\n",
    "                # s = s + (1-group['alpha']) * grad * grad\n",
    "                s.addcmul_(grad, grad, value=1-group['alpha'])\n",
    "\n",
    "                avg = s.sqrt().add_(group['eps'])\n",
    "                \n",
    "                # p.add_(grad / avg, alpha=-group['lr'])\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.RMSprop([W1], lr=lr, alpha=0.99, weight_decay=0.01)\n",
    "opt_custom = RMSProp([W2], lr=lr, alpha=0.99, weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['step', 'square_avg']\n",
    "    assert torch.equal(state1['square_avg'], state2['square_avg'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e4f31",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Idea:** Combine momentum (first moment) with RMSprop (second moment). Add bias correction for early steps.\n",
    "\n",
    "```\n",
    "v = β1 * v + (1 - β1) * g          # first moment (direction)\n",
    "s = β2 * s + (1 - β2) * g²         # second moment (scaling)\n",
    "\n",
    "v_corrected = v / (1 - β1^t)       # bias correction\n",
    "s_corrected = s / (1 - β2^t)\n",
    "\n",
    "θ = θ - lr * v_corrected / (√s_corrected + ε)\n",
    "```\n",
    "\n",
    "Typical: β1 = 0.9, β2 = 0.999, ε = 1e-8.\n",
    "\n",
    "Bias correction compensates for zero initialization. After ~1000 steps, correction ≈ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7111ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # Lazy Init\n",
    "                if p not in self.state:\n",
    "                    self.state[p] = {\n",
    "                        'step': 0,\n",
    "                        'exp_avg': torch.zeros_like(p),\n",
    "                        'exp_avg_sq': torch.zeros_like(p),\n",
    "                    }\n",
    "                self.state[p]['step'] += 1\n",
    "\n",
    "                # Weight Decay\n",
    "                grad = p.grad\n",
    "                if group['weight_decay'] != 0.0:\n",
    "                    # equivalent to: loss = ... + weight_decay/2 * W**2\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])  # not in place!\n",
    "\n",
    "                # Update v\n",
    "                # v = B1 * v + (1-B1) * g\n",
    "                v = self.state[p]['exp_avg']\n",
    "                v.lerp_(grad, 1 - group['betas'][0])\n",
    "\n",
    "                # Update s\n",
    "                # s = B2 * s + (1-B2) * g**2\n",
    "                s = self.state[p]['exp_avg_sq']\n",
    "                s.mul_(group['betas'][1])\n",
    "                s.addcmul_(grad, grad, value=1-group['betas'][1])\n",
    "\n",
    "                # Correction\n",
    "                # Somewhat convoluted way to do:\n",
    "                # v_corrected = v / (1-B1**t)\n",
    "                # s_corrected = s / (1-B2**t)\n",
    "                # p = p - lr * v_corrected / (sqrt(s_corrected)+eps)\n",
    "                t = self.state[p]['step']\n",
    "                bias1 = 1-group['betas'][0]**t\n",
    "                bias2 = 1-group['betas'][1]**t\n",
    "                bias2_sqrt = bias2**0.5\n",
    "                denom = (s.sqrt() / bias2_sqrt).add_(group['eps'])\n",
    "                p.data.addcdiv_(v, denom, value=-group['lr'] / bias1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d196c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Forward + backward\n",
    "x = torch.randn(16, 32)\n",
    "target = torch.randn(16, 64)\n",
    "\n",
    "# Create identical weights\n",
    "W1 = torch.randn(64, 32, requires_grad=True)\n",
    "W2 = W1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Params\n",
    "lr = 0.02\n",
    "\n",
    "# Optimizers\n",
    "opt_torch = torch.optim.Adam([W1], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "opt_custom = Adam([W2], lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "for i in range(20):\n",
    "    opt_torch.zero_grad()\n",
    "    opt_custom.zero_grad()\n",
    "    loss1 = ((x @ W1.T - target) ** 2).mean()\n",
    "    loss2 = ((x @ W2.T - target) ** 2).mean()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "\n",
    "    opt_torch.step()\n",
    "    opt_custom.step()\n",
    "\n",
    "    state1 = opt_torch.state[W1]\n",
    "    state2 = opt_custom.state[W2]\n",
    "    assert list(state1.keys()) == ['step', 'exp_avg', 'exp_avg_sq']\n",
    "    assert state1['step'] == state2['step']\n",
    "    assert torch.equal(state1['exp_avg'], state2['exp_avg'])\n",
    "    assert torch.equal(state1['exp_avg_sq'], state2['exp_avg_sq'])\n",
    "\n",
    "    weight_max_diff = (W1 - W2).abs().max().item()\n",
    "    assert weight_max_diff == 0.0\n",
    "    print(f\"Diff {weight_max_diff}\")\n",
    "\n",
    "print(f\"All good after {i+1} iterations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e61e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_max_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c98cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state2['exp_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2060d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
