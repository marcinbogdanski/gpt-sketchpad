{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Makemore Part 4: Backprop Ninja</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Manual backprop through a **MLP** model. Pen-and-paper derivations\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "letters = ['.'] + letters\n",
    "n_vocab = len(letters)\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5ce737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63733ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tok, block_size, names):\n",
    "    X, Y = [], []  # inputs and targets\n",
    "    for name in names:\n",
    "        name = '.'*block_size + name + '.'  # add start/stop tokens '..emma.'\n",
    "        for i in range(len(name) - block_size):\n",
    "            X.append(tok.encode(name[i:i+block_size]))\n",
    "            Y.append(tok.encode(name[i+block_size])[0])  # [0] to keep Y 1d tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length\n",
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tok, block_size, names[:n1])\n",
    "Xval, Yval = build_dataset(tok, block_size, names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(tok, block_size, names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af25a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, at, bt):\n",
    "    ex = torch.all(at == bt).item()\n",
    "    app = torch.allclose(at, bt)\n",
    "    maxdiff = (at - bt).abs().max().item()\n",
    "    print(f'{s:18s} | exat: {str(ex):5s} | approx: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab74a20",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9a74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Layers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "n_batch = 32\n",
    "n_embd = 10\n",
    "n_hid = 200\n",
    "\n",
    "# Model\n",
    "C = torch.randn((n_vocab, n_embd))                              # n_vocab, n_emb (embeddings)\n",
    "W1_kaiming_init = (5/3)/((n_embd*block_size)**0.5)              # tanh_gain / sqrt(fan_in)\n",
    "W1 = torch.randn((n_embd*block_size, n_hid)) * W1_kaiming_init  # n_seq*n_emb, n_hid\n",
    "b1 = torch.randn(n_hid)                      * 0.1              # n_hid\n",
    "bngain = torch.randn((1, n_hid))             * 0.1 + 1.0        # 1, n_hid\n",
    "bnbias = torch.randn((1, n_hid))             * 0.1              # 1, n_hid\n",
    "W2 = torch.randn((n_hid, n_vocab))           * 0.1              # n_hid, n_out\n",
    "b2 = torch.randn(n_vocab)                    * 0.1              # 1, n_out\n",
    "\n",
    "# Gather Params\n",
    "params = [C, W1, b1, bngain, bnbias, W2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# No gradient calculation\n",
    "bnmean1_running = torch.zeros((1, n_hid))\n",
    "bnvar1_running = torch.ones((1, n_hid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfaecd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, losses = [], []\n",
    "\n",
    "lr_schedule = [0.1]*100000 + [0.01]*100000\n",
    "num_epochs = len(lr_schedule)\n",
    "batch_size = 32\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb87d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06  0  3.466693639755249\n",
      "24.71  10000  2.5049779415130615\n",
      "41.14  20000  1.9582557678222656\n",
      "43.67  30000  1.9478200674057007\n",
      "42.93  40000  2.1745622158050537\n",
      "42.67  50000  2.1868133544921875\n",
      "43.68  60000  1.9040783643722534\n",
      "40.21  70000  1.9509446620941162\n",
      "45.13  80000  2.00536847114563\n",
      "41.18  90000  2.4943363666534424\n",
      "38.10  100000  2.0863988399505615\n",
      "41.10  110000  2.2291364669799805\n",
      "43.78  120000  2.1264493465423584\n",
      "43.08  130000  2.389422655105591\n",
      "41.33  140000  2.2911500930786133\n",
      "44.44  150000  2.2828786373138428\n",
      "43.70  160000  2.3709001541137695\n",
      "37.07  170000  1.9904111623764038\n",
      "50.55  180000  1.9067087173461914\n",
      "38.19  190000  1.9187291860580444\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_start = time.time()\n",
    "for _ in range(num_epochs):\n",
    "\n",
    "    # Random mini batch\n",
    "    batch_indices = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    x_batch = Xtr[batch_indices]\n",
    "    y_batch = Ytr[batch_indices]\n",
    "\n",
    "    # Forward Pass\n",
    "\n",
    "    # Embedding\n",
    "    emb = C[x_batch]                                  # n_batch, n_seq, n_emb\n",
    "    embcat = emb.view(-1, n_embd*block_size)          # n_batch, n_embd*block_size\n",
    "    # Linear 1\n",
    "    z1 = embcat @ W1 + b1                             # n_batch, n_hid\n",
    "    # Bachnorm 1\n",
    "    z1_mean = z1.mean(0, keepdim=True)\n",
    "    z1_var = z1.var(0, keepdim=True, unbiased=True)\n",
    "    z1_std_inv = (z1_var + 1e-5)**-0.5\n",
    "    zx = (z1 - z1_mean) * z1_std_inv\n",
    "    zz = bngain * zx + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean1_running = 0.999 * bnmean1_running + 0.001 * z1_mean\n",
    "        bnvar1_running = 0.999 * bnvar1_running + 0.001 * z1_var\n",
    "    # Tanh 1\n",
    "    h1 = torch.tanh(zz)\n",
    "    # Linear 2\n",
    "    logits = h1 @ W2 + b2                           # n_batch, n_vocab\n",
    "    # Cross Entropy Loss\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    # Backward Pass - Torch\n",
    "    #loss.backward()\n",
    "\n",
    "    # Backward Pass - Manual\n",
    "    grads = list(range(7))\n",
    "    with torch.no_grad():\n",
    "        # Cross Entropy\n",
    "        d_logits = F.softmax(logits, dim=1)\n",
    "        d_logits[range(n_batch), y_batch] -= 1\n",
    "        d_logits /= n_batch\n",
    "        # Linear 2\n",
    "        d_h1 = d_logits @ W2.T\n",
    "        d_W2 = h1.T @ d_logits\n",
    "        d_b2 = d_logits.sum(dim=0)\n",
    "        # Tanh 1        \n",
    "        d_zz = (1 - torch.tanh(zz)**2)  *   d_h1  # don't forget chain rule\n",
    "        # Batch Norm\n",
    "        d_bngain = (zx * d_zz).sum(dim=0, keepdim=True)\n",
    "        d_bnbias = d_zz.sum(dim=0, keepdim=True)\n",
    "        d_z1 = bngain * z1_std_inv / n_batch * (\n",
    "            n_batch * d_zz \n",
    "            - d_zz.sum(0) \n",
    "            - n_batch/(n_batch-1) * zx * (d_zz * zx).sum(0)\n",
    "        )\n",
    "        # Linear 1\n",
    "        d_embcat = d_z1 @ W1.T\n",
    "        d_W1 = embcat.T @ d_z1\n",
    "        d_b1 = d_z1.sum(dim=0)\n",
    "        # Embedding\n",
    "        d_emb = d_embcat.view(-1, block_size, n_embd)\n",
    "        d_C = torch.zeros_like(C)\n",
    "        d_C.index_add_(0, x_batch.view(-1), d_emb.view(-1, n_embd))\n",
    "        grads = [d_C, d_W1, d_b1, d_bngain, d_bnbias, d_W2, d_b2]\n",
    "\n",
    "    # Update\n",
    "    lr = lr_schedule[i]\n",
    "    for p, grad in zip(params, grads):\n",
    "        # p.data += -lr * p.grad   # Torch\n",
    "        p.data += -lr * grad      # Manual\n",
    "\n",
    "    # Stats\n",
    "    if i % 10000 == 0:\n",
    "        time_taken = time.time() - time_start\n",
    "        time_start = time.time()\n",
    "        print(f\"{time_taken:.2f}  {i}  {loss.item()}\")\n",
    "    iters.append(i)\n",
    "    losses.append(loss.item())\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather Params\n",
    "names = [\"d_C\", \"d_W1\", \"d_b1\", \"d_bngain\", \"d_bnbias\", \"d_W2\", \"d_b2\"]\n",
    "for param, grad, name in zip(params, grads, names):\n",
    "    print(name)\n",
    "    cmp(name, grad, param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a77ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train =  2.072300910949707\n",
      "eval =   2.1154792308807373\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(Xset, Yset):\n",
    "    # Embedding\n",
    "    emb = C[Xset]                                  # n_batch, n_seq, n_emb\n",
    "    embcat = emb.view(-1, n_embd*block_size)          # n_batch, n_embd*block_size\n",
    "    # Linear 1\n",
    "    z1 = embcat @ W1 + b1                             # n_batch, n_hid\n",
    "    # Bachnorm 1\n",
    "    z1_std_inv = (bnvar1_running + 1e-5)**-0.5\n",
    "    zx = (z1 - bnmean1_running) * z1_std_inv\n",
    "    zz = bngain * zx + bnbias\n",
    "    # Tanh 1\n",
    "    h1 = torch.tanh(zz)\n",
    "    # Linear 2\n",
    "    logits = h1 @ W2 + b2                           # n_batch, n_vocab\n",
    "    # Cross Entropy Loss\n",
    "    loss = F.cross_entropy(logits, Yset)\n",
    "    return loss.item()\n",
    "\n",
    "print(\"train = \", evaluate(Xtr, Ytr))    # ~2.12\n",
    "print(\"eval =  \", evaluate(Xval, Yval))  # ~2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7e8eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_name():\n",
    "    context = tok.encode('.'*block_size)\n",
    "    while True:\n",
    "        x = torch.tensor(context[-3:]).view(1, -1)   # n_batch=1, n_seq\n",
    "        # Forward Pass\n",
    "        emb = C[x]                                  # n_batch, n_seq, n_emb\n",
    "        embcat = emb.view(-1, n_embd*block_size)          # n_batch, n_embd*block_size\n",
    "        # Linear 1\n",
    "        z1 = embcat @ W1 + b1                             # n_batch, n_hid\n",
    "        # Bachnorm 1\n",
    "        z1_std_inv = (bnvar1_running + 1e-5)**-0.5\n",
    "        zx = (z1 - bnmean1_running) * z1_std_inv\n",
    "        zz = bngain * zx + bnbias\n",
    "        # Tanh 1\n",
    "        h1 = torch.tanh(zz)\n",
    "        # Linear 2\n",
    "        logits = h1 @ W2 + b2                           # n_batch, n_vocab\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        # Sample\n",
    "        sample = torch.multinomial(probs, 1).item()\n",
    "        context.append(sample)\n",
    "        # Break\n",
    "        if sample == 0:  # stop token\n",
    "            break\n",
    "\n",
    "    return tok.decode(context)[block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d4ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anuellyn.\n",
      "jamar.\n",
      "idushan.\n",
      "shan.\n",
      "silaylen.\n",
      "kemarce.\n",
      "man.\n",
      "emiah.\n",
      "nasildie.\n",
      "kani.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "for i in range(10):\n",
    "    print(sample_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
