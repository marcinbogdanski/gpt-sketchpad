{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Character Level MLP - Torch Autograd</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let's create a **bigram** model by **gradient descent** - a single linear layer pseudo neural network.\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results.\n",
    "\n",
    "References:\n",
    "\n",
    "- [Bengio et al. 2003 MLP language model paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Confirm the vocabulary is ASCII only\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "\n",
    "# Add start/stop/pad tokens - same for all\n",
    "letters = ['.'] + letters\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db5ce737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        assert isinstance(sequence, torch.Tensor)\n",
    "        assert sequence.ndim in [0, 1]\n",
    "        if sequence.ndim == 0:\n",
    "            return self.itos[sequence.item()]  # one char\n",
    "        else:\n",
    "            return ''.join([self.itos[i.item()] for i in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 0), ('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5), ('f', 6), ('g', 7), ('h', 8), ('i', 9)]\n",
      "[(0, '.'), (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e'), (6, 'f'), (7, 'g'), (8, 'h'), (9, 'i')]\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "print(list(tok.stoi.items())[:10])\n",
    "print(list(tok.itos.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "adb0e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 228146\n",
      "X torch.Size([228146, 3]),torch.int64:\n",
      "tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        [ 5, 13, 13],\n",
      "        [13, 13,  1],\n",
      "        [ 0,  0,  0],\n",
      "        [ 0,  0, 15],\n",
      "        [ 0, 15, 12],\n",
      "        [15, 12,  9],\n",
      "        [12,  9, 22]])\n",
      "Y torch.Size([228146]),torch.int64:\n",
      "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9])\n"
     ]
    }
   ],
   "source": [
    "blok_size = 3  # context length\n",
    "\n",
    "X, Y = [], []  # inputs and targets\n",
    "\n",
    "for name in names:\n",
    "    name = '.'*blok_size + name + '.'  # add start/stop tokens '..emma.'\n",
    "    for i in range(len(name) - blok_size):\n",
    "        X.append(tok.encode(name[i:i+blok_size]))\n",
    "        Y.append(tok.encode(name[i+blok_size])[0])  # [0] to keep Y 1d tensor\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "print(\"Num examples:\", len(X))\n",
    "print(f\"X {X.shape},{X.dtype}:\")\n",
    "print(X[:10])\n",
    "print(f\"Y {Y.shape},{Y.dtype}:\")\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a30a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e94116e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])    -> tensor(5)            ... -> e\n",
      "tensor([0, 0, 5])    -> tensor(13)           ..e -> m\n",
      "tensor([ 0,  5, 13]) -> tensor(13)           .em -> m\n",
      "tensor([ 5, 13, 13]) -> tensor(1)            emm -> a\n",
      "tensor([13, 13,  1]) -> tensor(0)            mma -> .\n",
      "tensor([0, 0, 0])    -> tensor(15)           ... -> o\n",
      "tensor([ 0,  0, 15]) -> tensor(12)           ..o -> l\n",
      "tensor([ 0, 15, 12]) -> tensor(9)            .ol -> i\n",
      "tensor([15, 12,  9]) -> tensor(22)           oli -> v\n",
      "tensor([12,  9, 22]) -> tensor(9)            liv -> i\n"
     ]
    }
   ],
   "source": [
    "# Mini batch:\n",
    "x_batch = X[:32]\n",
    "y_batch = Y[:32]\n",
    "\n",
    "for x, y in zip(x_batch[:10], y_batch[:10]):\n",
    "    print(f\"{str(x):<20} -> {str(y):<16}     {tok.decode(x)} -> {tok.decode(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "52318be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Layers\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn((27, 2), requires_grad=True)     # n_vocab, n_emb (embeddings)\n",
    "W1 = torch.randn((6, 100), requires_grad=True)   # n_seq+n_emb, n_hid1\n",
    "b1 = torch.randn((1, 100), requires_grad=True)   # 1, n_hid1\n",
    "W2 = torch.randn((100, 27), requires_grad=True)  # n_hid1, n_out\n",
    "b2 = torch.randn((1, 27), requires_grad=True)    # 1, n_out\n",
    "params = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "cc788989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.903022766113281\n",
      "3.795100688934326\n",
      "3.4425148963928223\n",
      "2.797386646270752\n",
      "2.9738969802856445\n",
      "2.947486400604248\n",
      "2.6208128929138184\n",
      "2.5312087535858154\n",
      "2.565312623977661\n",
      "2.579007387161255\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    # Random mini batch\n",
    "    batch_indices = torch.randint(0, X.shape[0], (32,))\n",
    "    x_batch = X[batch_indices]\n",
    "    y_batch = Y[batch_indices]\n",
    "\n",
    "    # Forward Pass\n",
    "    emb = C[x_batch]                            # n_batch, n_seq, n_emb\n",
    "    h1 = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # n_batch, n_hid1\n",
    "    logits = h1 @ W2 + b2                       # n_batch, n_vocab\n",
    "    loss = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "    if i % 100 == 0: print(loss.item())\n",
    "\n",
    "    # Backward Pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "# Expect loss to drop to ~0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7095, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "emb = C[X]                                  # n_batch, n_seq, n_emb\n",
    "h1 = torch.tanh(emb.view(-1, 6) @ W1 + b1)  # n_batch, n_hid1\n",
    "logits = h1 @ W2 + b2                       # n_batch, n_vocab\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "print(loss)  # ~2.71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa388739",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0a2fefef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.1366, grad_fn=<MeanBackward0>)\n",
      "PyTorch softmax/cross_entropy seem correct! :)\n"
     ]
    }
   ],
   "source": [
    "def verify_softmax_and_cross_entropy():\n",
    "    def softmax(logits):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        max_ = torch.max(logits, dim=-1, keepdim=True)[0]\n",
    "        exp = torch.exp(logits - max_)\n",
    "        exp_sum = torch.sum(exp, dim=-1, keepdim=True)\n",
    "        return exp / exp_sum\n",
    "\n",
    "    def only_cross_entropy(y_hat, correct_target_idx):\n",
    "        \"\"\"Compute the cross-entropy loss. Equivalent to neg log likelihood.\"\"\"\n",
    "        target_class_prob = y_hat[torch.arange(len(y_hat)), correct_target_idx]    # n_batch\n",
    "        ce_loss = -1 * torch.log(target_class_prob)\n",
    "        return ce_loss\n",
    "\n",
    "    def fused_cross_entropy(logits, correct_target_idx):\n",
    "        \"\"\"Softmax fused with cross_entropy. Matches F.cross_entropy\"\"\"\n",
    "        y_hat = softmax(logits)\n",
    "        ce_loss = only_cross_entropy(y_hat, correct_target_idx)\n",
    "        return ce_loss.mean()\n",
    "    \n",
    "    # Rand init\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Init Layers\n",
    "    C = torch.randn((27, 2), requires_grad=True)     # n_vocab, n_emb (embeddings)\n",
    "    W1 = torch.randn((6, 100), requires_grad=True)   # n_seq+n_emb, n_hid1\n",
    "    b1 = torch.randn((1, 100), requires_grad=True)   # 1, n_hid1\n",
    "    W2 = torch.randn((100, 27), requires_grad=True)  # n_hid1, n_out\n",
    "    b2 = torch.randn((1, 27), requires_grad=True)    # 1, n_out\n",
    "\n",
    "    # Mini batch:\n",
    "    x_batch = X[:12]\n",
    "    y_batch = Y[:12]\n",
    "\n",
    "    # Embed inputs\n",
    "    emb = C[x_batch]  # n_batch, n_seq, n_emb\n",
    "\n",
    "    # First layer\n",
    "    z1 = emb.view(-1, 6) @ W1 + b1  # n_batch, n_hid1\n",
    "    h1 = torch.tanh(z1)             # n_batch, n_hid1\n",
    "\n",
    "    # Output layer\n",
    "    logits = h1 @ W2 + b2   # n_batch, n_vocab\n",
    "\n",
    "    probs = softmax(logits)                # n_batch, n_vocab\n",
    "    probs_2 = torch.softmax(logits, -1)    # Equivalently\n",
    "    assert torch.allclose(probs, probs_2)\n",
    "\n",
    "    loss = fused_cross_entropy(logits, y_batch)  # scalar\n",
    "    loss_2 = F.cross_entropy(logits, y_batch)    # equivalent\n",
    "    assert torch.allclose(loss, loss_2)\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "    print(\"PyTorch softmax/cross_entropy seem correct! :)\")\n",
    "\n",
    "verify_softmax_and_cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c98bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
