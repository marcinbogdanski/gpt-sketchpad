{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b55dd8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold; font-size: 36px;\">Makemore Part 4: Backprop Ninja</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ee76",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Manual backprop through a **MLP** model. Pen-and-paper derivations\n",
    "\n",
    "Inspired by Karpathy [Neural Networks: Zero-to-Hero](https://github.com/karpathy/nn-zero-to-hero). \n",
    "We are using the same [names.txt](https://github.com/karpathy/makemore/blob/master/names.txt) as in Zero to Hero so we can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96879942",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f0a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4595b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cafa4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num names: 32033\n",
      "Example names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Min length: 2\n",
      "Max length: 15\n"
     ]
    }
   ],
   "source": [
    "with open('../data/names.txt', 'r') as f:\n",
    "    names = f.read().splitlines()\n",
    "print(\"Num names:\", len(names))\n",
    "print(\"Example names:\", names[:10])\n",
    "print(\"Min length:\", min(len(name) for name in names))\n",
    "print(\"Max length:\", max(len(name) for name in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7edf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "letters = sorted(list(set(''.join(names))))\n",
    "letters = ['.'] + letters\n",
    "n_vocab = len(letters)\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5ce737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        assert isinstance(vocab, list)\n",
    "        assert all(isinstance(v, str) for v in vocab)\n",
    "        assert all(len(v) == 1 for v in vocab)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi[s] for s in text]\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        if isinstance(sequence, list):\n",
    "            return ''.join([self.itos[i] for i in sequence])\n",
    "        elif isinstance(sequence, torch.Tensor):\n",
    "            assert sequence.ndim in [0, 1]\n",
    "            if sequence.ndim == 0:\n",
    "                return self.itos[sequence.item()]  # one char\n",
    "            else:\n",
    "                return ''.join([self.itos[i.item()] for i in sequence])\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type(sequence)} not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63733ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tok, block_size, names):\n",
    "    X, Y = [], []  # inputs and targets\n",
    "    for name in names:\n",
    "        name = '.'*block_size + name + '.'  # add start/stop tokens '..emma.'\n",
    "        for i in range(len(name) - block_size):\n",
    "            X.append(tok.encode(name[i:i+block_size]))\n",
    "            Y.append(tok.encode(name[i+block_size])[0])  # [0] to keep Y 1d tensor\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3  # context length\n",
    "tok = Tokenizer(vocab=letters)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(tok, block_size, names[:n1])\n",
    "Xval, Yval = build_dataset(tok, block_size, names[n1:n2])\n",
    "Xtest, Ytest = build_dataset(tok, block_size, names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af25a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, at, bt):\n",
    "    ex = torch.all(at == bt).item()\n",
    "    app = torch.allclose(at, bt)\n",
    "    maxdiff = (at - bt).abs().max().item()\n",
    "    print(f'{s:18s} | exat: {str(ex):5s} | approx: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52318be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Layers\n",
    "# NOTE: PyTorch F.cross_entropy uses multiple optimizations casuing slight\n",
    "# numerical differences. This \"lucky seed\" makes our manual and PyTorch losses match.\n",
    "# NOTE: instead of 1/x use x**-0.5, PyTorch kernels are not reproducible with 1/x\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "n_batch = 32\n",
    "n_embd = 10\n",
    "n_hid = 64\n",
    "\n",
    "# Model\n",
    "# NOTE: Init all params to non-zero so we don't mask gradient issues\n",
    "C = torch.randn((n_vocab, n_embd))                              # n_vocab, n_emb (embeddings)\n",
    "W1_kaiming_init = (5/3)/((n_embd*block_size)**0.5)              # tanh_gain / sqrt(fan_in)\n",
    "W1 = torch.randn((n_embd*block_size, n_hid)) * W1_kaiming_init  # n_seq*n_emb, n_hid\n",
    "b1 = torch.randn(n_hid)                      * 0.1              # n_hid\n",
    "bngain = torch.randn((1, n_hid))             * 0.1 + 1.0        # 1, n_hid\n",
    "bnbias = torch.randn((1, n_hid))             * 0.1              # 1, n_hid\n",
    "W2 = torch.randn((n_hid, n_vocab))           * 0.1              # n_hid, n_out\n",
    "b2 = torch.randn(n_vocab)                    * 0.1              # 1, n_out\n",
    "\n",
    "# Gather Params\n",
    "params = [C, W1, b1, bngain, bnbias, W2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Random mini batch\n",
    "batch_indices = torch.randint(0, Xtr.shape[0], (n_batch,))\n",
    "x_batch = Xtr[batch_indices]\n",
    "y_batch = Ytr[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc788989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch loss: 3.3894288539886475\n",
      "              3.3894288539886475\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "\n",
    "# Embedding\n",
    "emb = C[x_batch]                                  # n_batch, n_seq, n_emb\n",
    "embcat = emb.view(-1, n_embd*block_size)          # n_batch, n_embd*block_size\n",
    "\n",
    "# Linear 1\n",
    "z1 = embcat @ W1 + b1                             # n_batch, n_hid\n",
    "\n",
    "# Bachnorm 1\n",
    "# zx = (x - x_mean) / (x_var + 1e-5)**0.5\n",
    "# bn(x) = zx * gain + bias\n",
    "# Batchnorm 1 - mean\n",
    "z1_sum = z1.sum(dim=0, keepdim=True)              # 1, n_hid\n",
    "z1_mean = z1_sum / n_batch                        # 1, n_hid\n",
    "# Batchnorm 1 - var\n",
    "z1_diff = z1 - z1_mean                            # n_batch, n_hid\n",
    "z1_diff_2 = z1_diff**2.0                          # n_batch, n_hid\n",
    "z1_diff_sum = z1_diff_2.sum(dim=0, keepdim=True)  # 1, n_hid\n",
    "z1_var = z1_diff_sum * (n_batch-1)**-1            # 1, n_hid\n",
    "# Batchnorm 1 - correction\n",
    "z1_var_p_eps = z1_var + 1e-5                      # 1, n_hid\n",
    "z1_var_sqrt_inv = z1_var_p_eps**-0.5              # 1, n_hid\n",
    "zx = z1_diff * z1_var_sqrt_inv                    # n_batch, n_hid\n",
    "# Batchnorm 1 - scaling\n",
    "zz = bngain * zx + bnbias                         # n_batch, n_hid\n",
    "\n",
    "# Tanh 1\n",
    "# tanh(x) = (torch.exp(2*x) - 1.0) / (torch.exp(2*x) + 1)\n",
    "zz_2 = 2*zz                                 # n_batch, n_hid\n",
    "zz_2_exp = zz_2.exp()                       # n_batch, n_hid\n",
    "zz_2_exp_m1 = zz_2_exp - 1.0                # n_batch, n_hid\n",
    "zz_2_exp_p1 = zz_2_exp + 1.0                # n_batch, n_hid\n",
    "zz_2_exp_p1_inv = zz_2_exp_p1**-1           # n_batch, n_hid\n",
    "h1 = zz_2_exp_m1 * zz_2_exp_p1_inv          # n_batch, n_hid\n",
    "\n",
    "# Linear 2\n",
    "logits = h1 @ W2 + b2                       # n_batch, n_vocab\n",
    "\n",
    "# Softmax\n",
    "logits_max = logits.max(dim=-1, keepdim=True).values      # n_batch, 1\n",
    "logits_diff = logits - logits_max                         # n_batch, n_vocab\n",
    "logits_exp = logits_diff.exp()                            # n_batch, n_vocab\n",
    "logits_exp_sum = logits_exp.sum(dim=-1, keepdim=True)     # n_batch, 1\n",
    "logits_exp_sum_inv = logits_exp_sum**-1                   # n_batch, 1\n",
    "probs = logits_exp * logits_exp_sum_inv                   # n_batch, n_vocab\n",
    "\n",
    "# Cross Entropy\n",
    "log_probs = probs.log()                                         # n_batch, n_vocab\n",
    "log_probs_target = log_probs[range(n_batch), y_batch]           # n_batch\n",
    "loss = -log_probs_target.sum(dim=0, keepdim=True) / n_batch\n",
    "\n",
    "# PyTorch Backward Pass\n",
    "for p in params:\n",
    "    p.grad = None\n",
    "for t in [\n",
    "        emb, embcat,\n",
    "        z1,\n",
    "        z1_sum, z1_mean,\n",
    "        z1_diff, z1_diff_2, z1_diff_sum, z1_var,\n",
    "        z1_var_p_eps, z1_var_sqrt_inv, zx,\n",
    "        zz,\n",
    "        zz_2, zz_2_exp, zz_2_exp_m1, zz_2_exp_p1, zz_2_exp_p1_inv, h1,\n",
    "        logits,\n",
    "        logits_max, logits_diff, logits_exp, logits_exp_sum, logits_exp_sum_inv, probs,\n",
    "        log_probs, log_probs_target, loss\n",
    "    ]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # PyTorch loss: 3.3894288539886475\n",
    "    print(f\"PyTorch loss: {F.cross_entropy(logits, y_batch).item()}\")\n",
    "    print(f\"              {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da356ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # Cross Entropy\n",
    "    d_loss = torch.tensor(1.0)\n",
    "    d_log_probs_target = -1 * torch.ones(n_batch) * (1/n_batch * d_loss)\n",
    "    d_log_probs = torch.zeros_like(log_probs)\n",
    "    d_log_probs[range(n_batch), y_batch] = d_log_probs_target\n",
    "\n",
    "    # Softmax\n",
    "    d_probs = (1 / probs) * d_log_probs    # deriv of ln * chain rule\n",
    "    d_logits_exp_sum_inv = (logits_exp * d_probs).sum(dim=-1, keepdim=True)\n",
    "    d_logits_exp_sum = -1 * logits_exp_sum**-2 * d_logits_exp_sum_inv\n",
    "    d_logits_exp = torch.ones_like(logits_exp) * d_logits_exp_sum\n",
    "    d_logits_exp += logits_exp_sum_inv * d_probs  # second gradient path\n",
    "    d_logits_diff = logits_exp * d_logits_exp\n",
    "    d_logits_max = -d_logits_diff.sum(dim=-1, keepdim=True)   # n_batch, 1\n",
    "    d_logits = d_logits_diff.clone()  # logits_diff path\n",
    "    logits_maxi = logits.max(dim=-1).indices\n",
    "    d_logits[range(n_batch), logits_maxi] += d_logits_max[range(n_batch), 0]   # max path\n",
    "\n",
    "    # Linear 2\n",
    "    d_h1 = d_logits @ W2.T\n",
    "    d_W2 = h1.T @ d_logits\n",
    "    d_b2 = d_logits.sum(dim=0)\n",
    "\n",
    "    # Tanh 1\n",
    "    d_zz_2_exp_p1_inv = zz_2_exp_m1 * d_h1\n",
    "    d_zz_2_exp_p1 = -1 * zz_2_exp_p1**-2 * d_zz_2_exp_p1_inv\n",
    "    d_zz_2_exp_m1 = zz_2_exp_p1_inv * d_h1\n",
    "    d_zz_2_exp = d_zz_2_exp_p1 + d_zz_2_exp_m1\n",
    "    d_zz_2 = zz_2_exp * d_zz_2_exp\n",
    "    d_zz = 2 * d_zz_2\n",
    "\n",
    "    # Batch Norm 1 - scaling\n",
    "    d_bngain = (zx * d_zz).sum(dim=0, keepdim=True)\n",
    "    d_bnbias = d_zz.sum(dim=0, keepdim=True)\n",
    "    # Batch Norm 1 - correction\n",
    "    d_zx = torch.ones_like(zx) * bngain * d_zz\n",
    "    d_z1_var_sqrt_inv = (z1_diff * d_zx).sum(dim=0, keepdim=True)\n",
    "    d_z1_var_p_eps = -0.5*z1_var_p_eps**-1.5 * d_z1_var_sqrt_inv\n",
    "    # Batch Norm 1 - var\n",
    "    d_z1_var = d_z1_var_p_eps.clone()\n",
    "    d_z1_diff_sum = (n_batch-1)**-1 * d_z1_var\n",
    "    d_z1_diff_2 = torch.ones_like(z1_diff_2) * d_z1_diff_sum\n",
    "    d_z1_diff = 2 * z1_diff * d_z1_diff_2\n",
    "    d_z1_diff += d_zx * z1_var_sqrt_inv  # second path\n",
    "    # Batch Norm 1 - mean\n",
    "    d_z1_mean = -1 * d_z1_diff.sum(dim=0, keepdim=True)\n",
    "    d_z1_sum = 1/n_batch * d_z1_mean\n",
    "    d_z1 = torch.ones_like(z1) * d_z1_sum\n",
    "    d_z1 += d_z1_diff\n",
    "\n",
    "    # Linear 1\n",
    "    d_embcat = d_z1 @ W1.T\n",
    "    d_W1 = embcat.T @ d_z1\n",
    "    d_b1 = d_z1.sum(dim=0)\n",
    "\n",
    "    # Embedding\n",
    "    d_emb = d_embcat.view(-1, block_size, n_embd)\n",
    "    d_C = torch.zeros_like(C)\n",
    "    d_C.index_add_(0, x_batch.view(-1), d_emb.view(-1, n_embd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9196d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss               | exat: True  | approx: True  | maxdiff: 0.0\n",
      "log_probs_target   | exat: True  | approx: True  | maxdiff: 0.0\n",
      "log_probs          | exat: True  | approx: True  | maxdiff: 0.0\n",
      "probs              | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp_sum_inv | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp_sum     | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_exp         | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_diff        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits_max         | exat: True  | approx: True  | maxdiff: 0.0\n",
      "logits             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "h1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "W2                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "b2                 | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy\n",
    "cmp(\"loss\", d_loss, loss.grad)\n",
    "cmp(\"log_probs_target\", d_log_probs_target, log_probs_target.grad)\n",
    "cmp(\"log_probs\", d_log_probs, log_probs.grad)\n",
    "\n",
    "# Softmax\n",
    "cmp(\"probs\", d_probs, probs.grad)\n",
    "cmp(\"logits_exp_sum_inv\", d_logits_exp_sum_inv, logits_exp_sum_inv.grad)\n",
    "cmp(\"logits_exp_sum\", d_logits_exp_sum, logits_exp_sum.grad)\n",
    "cmp(\"logits_exp\", d_logits_exp, logits_exp.grad)\n",
    "cmp(\"logits_diff\", d_logits_diff, logits_diff.grad)\n",
    "cmp(\"logits_max\", d_logits_max, logits_max.grad)\n",
    "cmp(\"logits\", d_logits, logits.grad)\n",
    "\n",
    "# Linear 2\n",
    "cmp(\"h1\", d_h1, h1.grad)\n",
    "cmp(\"W2\", d_W2, W2.grad)\n",
    "cmp(\"b2\", d_b2, b2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6771196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zz_2_exp_p1_inv    | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp_p1        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp_m1        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2_exp           | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz_2               | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zz                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "bngain             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "bnbias             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "zx                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var_sqrt_inv    | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var_p_eps       | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_var             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff_sum        | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff_2          | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_diff            | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_mean            | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1_sum             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "z1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "embcat             | exat: True  | approx: True  | maxdiff: 0.0\n",
      "W1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "b1                 | exat: True  | approx: True  | maxdiff: 0.0\n",
      "emb                | exat: True  | approx: True  | maxdiff: 0.0\n",
      "C                  | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Tanh 1\n",
    "cmp(\"zz_2_exp_p1_inv\", d_zz_2_exp_p1_inv, zz_2_exp_p1_inv.grad)\n",
    "cmp(\"zz_2_exp_p1\", d_zz_2_exp_p1, zz_2_exp_p1.grad)\n",
    "cmp(\"zz_2_exp_m1\", d_zz_2_exp_m1, zz_2_exp_m1.grad)\n",
    "cmp(\"zz_2_exp\", d_zz_2_exp, zz_2_exp.grad)\n",
    "cmp(\"zz_2\", d_zz_2, zz_2.grad)\n",
    "\n",
    "# Batch Norm 1 - scaling\n",
    "cmp(\"zz\", d_zz, zz.grad)\n",
    "cmp(\"bngain\", d_bngain, bngain.grad)\n",
    "cmp(\"bnbias\", d_bnbias, bnbias.grad)\n",
    "# Batch Norm 1 - correction\n",
    "cmp(\"zx\", d_zx, zx.grad)\n",
    "cmp(\"z1_var_sqrt_inv\", d_z1_var_sqrt_inv, z1_var_sqrt_inv.grad)\n",
    "cmp(\"z1_var_p_eps\", d_z1_var_p_eps, z1_var_p_eps.grad)\n",
    "# Batch Norm 1 - var\n",
    "cmp(\"z1_var\", d_z1_var, z1_var.grad)\n",
    "cmp(\"z1_diff_sum\", d_z1_diff_sum, z1_diff_sum.grad)\n",
    "cmp(\"z1_diff_2\", d_z1_diff_2, z1_diff_2.grad)\n",
    "cmp(\"z1_diff\", d_z1_diff, z1_diff.grad)\n",
    "# Batch Norm 1 - mean\n",
    "cmp(\"z1_mean\", d_z1_mean, z1_mean.grad)\n",
    "cmp(\"z1_sum\", d_z1_sum, z1_sum.grad)\n",
    "cmp(\"z1\", d_z1, z1.grad)\n",
    "\n",
    "# Linear 1\n",
    "cmp(\"embcat\", d_embcat, embcat.grad)\n",
    "cmp(\"W1\", d_W1, W1.grad)\n",
    "cmp(\"b1\", d_b1, b1.grad)\n",
    "\n",
    "# Embedding\n",
    "cmp(\"emb\", d_emb, emb.grad)\n",
    "cmp(\"C\", d_C, C.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da550b3",
   "metadata": {},
   "source": [
    "# Cross Entropy - derive better graddient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad473204",
   "metadata": {},
   "source": [
    "Loss:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "Softmax:\n",
    "$$\n",
    "p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Gradient\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}} = \\mathbf{p} - \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f7bcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_xxx           | exat: True  | approx: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Forwawrd - before\n",
    "\n",
    "# # Softmax\n",
    "# logits_max = logits.max(dim=-1, keepdim=True).values      # n_batch, 1\n",
    "# logits_diff = logits - logits_max                         # n_batch, n_vocab\n",
    "# logits_exp = logits_diff.exp()                            # n_batch, n_vocab\n",
    "# logits_exp_sum = logits_exp.sum(dim=-1, keepdim=True)     # n_batch, 1\n",
    "# logits_exp_sum_inv = logits_exp_sum**-1                   # n_batch, 1\n",
    "# probs = logits_exp * logits_exp_sum_inv                   # n_batch, n_vocab\n",
    "\n",
    "# # Cross Entropy\n",
    "# log_probs = probs.log()                                         # n_batch, n_vocab\n",
    "# log_probs_target = log_probs[range(n_batch), y_batch]           # n_batch\n",
    "# loss = -log_probs_target.sum(dim=0, keepdim=True) / n_batch\n",
    "\n",
    "# Forward - new\n",
    "with torch.no_grad():\n",
    "    loss_xxx = F.cross_entropy(logits, y_batch)\n",
    "\n",
    "cmp(\"loss_xxx\", loss_xxx, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46df7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits             | exat: False | approx: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward - before\n",
    "\n",
    "# # Cross Entropy\n",
    "# d_loss = torch.tensor(1.0)\n",
    "# d_log_probs_target = -1 * torch.ones(n_batch) * (1/n_batch * d_loss)\n",
    "# d_log_probs = torch.zeros_like(log_probs)\n",
    "# d_log_probs[range(n_batch), y_batch] = d_log_probs_target\n",
    "\n",
    "# # Softmax\n",
    "# d_probs = (1 / probs) * d_log_probs    # deriv of ln * chain rule\n",
    "# d_logits_exp_sum_inv = (logits_exp * d_probs).sum(dim=-1, keepdim=True)\n",
    "# d_logits_exp_sum = -1 * logits_exp_sum**-2 * d_logits_exp_sum_inv\n",
    "# d_logits_exp = torch.ones_like(logits_exp) * d_logits_exp_sum\n",
    "# d_logits_exp += logits_exp_sum_inv * d_probs  # second gradient path\n",
    "# d_logits_diff = logits_exp * d_logits_exp\n",
    "# d_logits_max = -d_logits_diff.sum(dim=-1, keepdim=True)   # n_batch, 1\n",
    "# d_logits = d_logits_diff.clone()  # logits_diff path\n",
    "# logits_maxi = logits.max(dim=-1).indices\n",
    "# d_logits[range(n_batch), logits_maxi] += d_logits_max[range(n_batch), 0]   # max path\n",
    "\n",
    "# Backward - new\n",
    "with torch.no_grad():\n",
    "    d_logits_xxx = F.softmax(logits, dim=1)\n",
    "    d_logits_xxx[range(n_batch), y_batch] -= 1\n",
    "    d_logits_xxx /= n_batch\n",
    "\n",
    "cmp(\"logits\", d_logits_xxx, logits.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d_logits_xxx.detach(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5151ff",
   "metadata": {},
   "source": [
    "# Tanh - derive better graddient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d827b85",
   "metadata": {},
   "source": [
    "Tanh\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "Gradient\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\tanh(x)}{\\partial x} = 1 - \\tanh^2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d68b162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1_xxx             | exat: False | approx: True  | maxdiff: 1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "# Forward - old\n",
    "\n",
    "# # Tanh 1\n",
    "# # tanh(x) = (torch.exp(2*x) - 1.0) / (torch.exp(2*x) + 1)\n",
    "# zz_2 = 2*zz                                 # n_batch, n_hid\n",
    "# zz_2_exp = zz_2.exp()                       # n_batch, n_hid\n",
    "# zz_2_exp_m1 = zz_2_exp - 1.0                # n_batch, n_hid\n",
    "# zz_2_exp_p1 = zz_2_exp + 1.0                # n_batch, n_hid\n",
    "# zz_2_exp_p1_inv = zz_2_exp_p1**-1           # n_batch, n_hid\n",
    "# h1 = zz_2_exp_m1 * zz_2_exp_p1_inv          # n_batch, n_hid\n",
    "\n",
    "# Forward - new\n",
    "with torch.no_grad():\n",
    "    h1_xxx = torch.tanh(zz)\n",
    "\n",
    "cmp(\"h1_xxx\", h1_xxx, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec81154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_zz_xxx           | exat: False | approx: True  | maxdiff: 1.6298145055770874e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward - old\n",
    "\n",
    "# # Tanh 1\n",
    "# d_zz_2_exp_p1_inv = zz_2_exp_m1 * d_h1\n",
    "# d_zz_2_exp_p1 = -1 * zz_2_exp_p1**-2 * d_zz_2_exp_p1_inv\n",
    "# d_zz_2_exp_m1 = zz_2_exp_p1_inv * d_h1\n",
    "# d_zz_2_exp = d_zz_2_exp_p1 + d_zz_2_exp_m1\n",
    "# d_zz_2 = zz_2_exp * d_zz_2_exp\n",
    "# d_zz = 2 * d_zz_2\n",
    "\n",
    "# Backward - new\n",
    "\n",
    "d_zz_xxx = (1 - torch.tanh(zz)**2)  *   d_h1  # don't forget chain rule\n",
    "cmp(\"d_zz_xxx\", d_zz_xxx, zz.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd697297",
   "metadata": {},
   "source": [
    "# Batch Norm - derive better graddient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "957c8b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.shape, d_zz_22.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70f317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward - old\n",
    "\n",
    "# # Bachnorm 1\n",
    "# # zx = (x - x_mean) / (x_var + 1e-5)**0.5\n",
    "# # bn(x) = zx * gain + bias\n",
    "# # Batchnorm 1 - mean\n",
    "# z1_sum = z1.sum(dim=0, keepdim=True)              # 1, n_hid\n",
    "# z1_mean = z1_sum / n_batch                        # 1, n_hid\n",
    "# # Batchnorm 1 - var\n",
    "# z1_diff = z1 - z1_mean                            # n_batch, n_hid\n",
    "# z1_diff_2 = z1_diff**2.0                          # n_batch, n_hid\n",
    "# z1_diff_sum = z1_diff_2.sum(dim=0, keepdim=True)  # 1, n_hid\n",
    "# z1_var = z1_diff_sum * (n_batch-1)**-1            # 1, n_hid\n",
    "# # Batchnorm 1 - correction\n",
    "# z1_var_p_eps = z1_var + 1e-5                      # 1, n_hid\n",
    "# z1_var_sqrt_inv = z1_var_p_eps**-0.5              # 1, n_hid\n",
    "# zx = z1_diff * z1_var_sqrt_inv                    # n_batch, n_hid\n",
    "# # Batchnorm 1 - scaling\n",
    "# zz = bngain * zx + bnbias                         # n_batch, n_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911c2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-sketchpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
